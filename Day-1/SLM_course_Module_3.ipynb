{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b956ACVlxf1w"
      },
      "outputs": [],
      "source": [
        "class GPTConfig:\n",
        "    def __init__(self):\n",
        "        self.vocab_size = meta[\"vocab_size\"]  # ~32,000 for BanglaT5\n",
        "        self.block_size = 128                 # Context length\n",
        "        self.n_layer = 8                      # Number of transformer blocks\n",
        "        self.n_head = 8                       # Number of attention heads\n",
        "        self.n_embd = 512                     # Embedding dimension\n",
        "        self.dropout = 0.1                    # Regularization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_parameters():\n",
        "    vocab_size = 32100\n",
        "    n_embd = 512\n",
        "    n_layer = 8\n",
        "    n_head = 8\n",
        "\n",
        "    # Token embeddings\n",
        "    token_emb_params = vocab_size * n_embd\n",
        "\n",
        "    # Position embeddings\n",
        "    pos_emb_params = 128 * n_embd  # block_size * n_embd\n",
        "\n",
        "    # Each transformer block\n",
        "    # Attention: 4 linear layers (Q, K, V, proj)\n",
        "    attn_params = 4 * (n_embd * n_embd)\n",
        "    # MLP: 2 linear layers (4*n_embd → n_embd)\n",
        "    mlp_params = n_embd * (4 * n_embd) + (4 * n_embd) * n_embd\n",
        "    # Layer norms (2 per block)\n",
        "    ln_params = 2 * n_embd\n",
        "\n",
        "    block_params = attn_params + mlp_params + ln_params\n",
        "    total_block_params = n_layer * block_params\n",
        "\n",
        "    # Final layer norm and head\n",
        "    final_ln_params = n_embd\n",
        "    head_params = n_embd * vocab_size\n",
        "\n",
        "    total = token_emb_params + pos_emb_params + total_block_params + final_ln_params + head_params\n",
        "\n",
        "    print(f\"Token Embeddings: {token_emb_params/1e6:.2f}M\")\n",
        "    print(f\"Position Embeddings: {pos_emb_params/1e6:.2f}M\")\n",
        "    print(f\"Transformer Blocks: {total_block_params/1e6:.2f}M\")\n",
        "    print(f\"Final Head: {head_params/1e6:.2f}M\")\n",
        "    print(f\"Total Parameters: {total/1e6:.2f}M\")\n",
        "\n",
        "calculate_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5qyHzglxpMN",
        "outputId": "1c8d4b15-0f03-4dae-a07b-f94a53373cce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token Embeddings: 16.44M\n",
            "Position Embeddings: 0.07M\n",
            "Transformer Blocks: 25.17M\n",
            "Final Head: 16.44M\n",
            "Total Parameters: 58.11M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, T5Model\n",
        "import torch\n",
        "\n",
        "# Load tokenizer and encoder-decoder model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/banglat5\")\n",
        "model = T5Model.from_pretrained(\"csebuetnlp/banglat5\")\n",
        "\n",
        "# Input text\n",
        "text = \"আমি বাংলায় কথা বলি\"\n",
        "\n",
        "# Tokenize\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "# Pass through encoder to get token embeddings\n",
        "with torch.no_grad():\n",
        "    encoder_outputs = model.encoder(**inputs)\n",
        "\n",
        "# Extract token-level vectors\n",
        "token_embeddings = encoder_outputs.last_hidden_state.squeeze(0)  # Shape: (seq_len, 512)\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze())\n",
        "\n",
        "# Show first 10 dims of each token's embedding\n",
        "for i, (token, vector) in enumerate(zip(tokens, token_embeddings)):\n",
        "    print(f\"{i+1}. Token: '{token}' → {vector}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNSJa7QhNZ8x",
        "outputId": "4f0b0377-1ebd-4234-b34d-7f64d30ecae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Token: '▁আমি' → tensor([ 3.8904e-02, -5.4560e-03,  1.5565e-01, -2.2271e-01,  2.1094e-01,\n",
            "         2.4145e-02,  4.8329e-02,  2.9140e-02, -8.5793e-03,  4.7105e-02,\n",
            "        -7.6535e-02,  6.3955e-03,  1.6493e-02, -5.8595e-02, -2.3978e-01,\n",
            "         4.2552e-02, -2.0625e-02,  9.3497e-02,  2.4191e-01,  1.5555e-01,\n",
            "        -1.0927e-01,  9.3477e-02, -1.3202e-02, -1.1819e-01, -6.0679e-02,\n",
            "        -2.9357e-02,  2.0075e-02, -6.9545e-02, -5.1615e-02, -2.4857e-02,\n",
            "        -4.6616e-02, -2.7636e-02,  2.4947e-02,  1.1155e-02, -7.5722e-02,\n",
            "         8.8244e-02, -6.9582e-02,  2.4739e-01,  2.8190e-02, -3.0684e-01,\n",
            "         1.1139e-01, -1.2097e-01,  7.5894e-02, -9.0453e-02,  1.9508e-01,\n",
            "        -1.6815e-01, -6.1600e-02,  6.5010e-02,  3.7247e-01, -6.2258e-02,\n",
            "         1.3307e-01, -2.2310e-01, -1.4692e-02,  1.9458e-01, -3.6996e-02,\n",
            "         1.4076e-01, -5.4206e-04, -6.9578e-02,  4.2316e-02,  1.8661e-02,\n",
            "        -4.9393e-02,  5.6061e-02,  4.8090e-02,  5.9825e-02, -4.6135e-02,\n",
            "        -1.9118e-01,  1.6947e-01, -7.2201e-02,  4.4335e-02, -9.5312e-02,\n",
            "         2.2621e-01, -1.6037e-02, -9.9980e-02, -1.0737e-02,  6.1558e-02,\n",
            "        -1.8755e-02, -7.1270e-02, -1.9837e-02, -1.6618e-02,  2.2065e-01,\n",
            "        -5.3930e-02, -3.0907e-02, -6.0213e-02,  1.8714e-01, -1.1997e-01,\n",
            "         4.0561e-02, -1.5593e-01,  8.0058e-02,  1.0722e-01, -1.2359e-01,\n",
            "         6.6710e-02, -3.4148e-02,  1.0228e-01, -6.2619e-05,  8.1158e-02,\n",
            "        -7.7635e-02, -1.3330e-01,  2.1655e-02,  7.7954e-02,  1.3704e-01,\n",
            "         8.5522e-02, -8.6665e-02, -2.2244e-01, -4.6578e-04,  2.5728e-02,\n",
            "        -8.2551e-02,  6.4183e-02, -1.5487e-01, -3.3612e-02,  1.4964e-01,\n",
            "         9.1439e-02,  8.3320e-02, -1.2702e-01,  1.1784e-01,  1.2757e-01,\n",
            "         1.6473e-01, -1.9537e-01, -1.9067e-02,  8.3203e-02,  1.2252e-01,\n",
            "         2.0278e-02, -7.9933e-02, -1.9159e-01,  5.8634e-02,  2.3809e-01,\n",
            "         1.0387e-01, -8.1688e-02, -2.3437e-01,  2.2435e-01, -2.1339e-01,\n",
            "        -5.2448e-02, -2.2166e-01,  1.9419e-01, -1.1454e-02,  1.7001e-01,\n",
            "        -8.5625e-02,  1.2139e-01, -1.8884e-02,  1.0534e-01,  6.8339e-02,\n",
            "         7.3254e-02, -1.6638e-01, -1.6115e-01,  2.0100e-01, -3.7133e-02,\n",
            "        -1.1127e-01, -4.1615e-02,  2.2811e-02, -1.4200e-04,  1.0093e-01,\n",
            "        -7.4681e-02, -5.8120e-02, -9.7429e-02, -7.6507e-02, -1.8242e-01,\n",
            "         5.6210e-02, -9.8799e-02, -1.6762e-02,  2.3277e-01, -1.3306e-02,\n",
            "         3.5142e-02,  1.0589e-01, -1.6188e-01, -5.7290e-02, -1.2637e-01,\n",
            "         4.7703e-01,  1.5006e-01,  9.7074e-02,  1.8459e-01, -1.4783e-01,\n",
            "        -4.4414e-02, -1.8654e-02,  1.6178e-01,  1.6533e-02, -2.4961e-01,\n",
            "         1.6130e-01, -1.2598e-01, -2.2247e-01, -1.0790e-02, -1.9221e-02,\n",
            "        -1.6618e-02,  5.2840e-02,  1.5220e-01, -1.1643e-01, -8.9601e-03,\n",
            "        -9.0605e-02, -1.0620e-01, -4.2304e-02,  1.0446e-02, -1.8614e-01,\n",
            "         1.1805e-01,  1.0238e-01,  1.0270e-01, -3.5217e-02, -9.3014e-02,\n",
            "        -1.4994e-01, -8.9900e-02,  4.6982e-02, -1.9751e-03,  1.0122e-01,\n",
            "        -4.9348e-02,  4.2227e-02, -1.1069e-01, -1.1247e-01, -5.4805e-02,\n",
            "         7.9256e-02, -8.0740e-02, -1.0969e-01,  9.3533e-02,  4.6782e-02,\n",
            "         1.0495e-02, -2.4199e-01,  1.5013e-01, -1.3850e-01, -8.8563e-02,\n",
            "         1.8744e-01, -9.1428e-02,  1.3518e-01,  4.9785e-02, -1.9856e-01,\n",
            "         2.4765e-02, -8.7141e-02,  1.1829e-01,  4.1187e-02,  6.1083e-02,\n",
            "         1.9513e-02,  3.6469e-03,  9.5049e-03, -3.3863e-02,  1.1141e-02,\n",
            "         8.7810e-02,  7.6743e-02,  7.0748e-02,  1.5811e-01,  6.5003e-02,\n",
            "        -1.5922e-02, -4.4404e-02,  8.0377e-02,  1.0602e-01,  3.8516e-02,\n",
            "         5.1161e-02,  1.3118e-01, -1.0861e-01, -2.5728e-02, -2.8982e-02,\n",
            "        -7.1895e-02,  1.3400e-01, -9.4877e-02,  2.6624e-02, -4.2273e-02,\n",
            "        -3.4690e-02, -1.7610e-01, -6.3318e-02,  2.0055e-02, -6.5617e-02,\n",
            "         1.0887e-01,  1.4737e-02, -6.7017e-02,  4.4666e-02,  2.1235e-02,\n",
            "         4.8332e-02, -1.5860e-01,  4.5413e-02,  4.0560e-02, -3.2799e-02,\n",
            "        -3.6221e-03, -1.7746e-01,  7.0678e-02,  2.9677e-02, -2.8343e-02,\n",
            "        -7.3950e-02,  1.4702e-01, -7.6767e-02, -7.9568e-02, -7.6793e-02,\n",
            "        -1.1993e-01,  9.8557e-03, -9.6268e-02, -1.8201e-02, -9.1854e-03,\n",
            "         6.1676e-02, -8.9250e-02,  4.5026e-02,  8.7756e-02, -5.1131e-03,\n",
            "        -1.7442e-01,  2.0632e-01, -1.0627e-01,  1.2466e-01,  1.1599e-01,\n",
            "        -4.0521e-02,  4.4897e-02, -6.3576e-02,  8.2174e-02,  1.9100e-01,\n",
            "         1.2184e-01,  3.4603e-03,  2.7920e-02, -2.4757e-02, -1.8841e-01,\n",
            "        -3.0229e-02, -7.0102e-02, -3.5813e-03,  5.7607e-02,  9.5269e-02,\n",
            "        -3.2871e-02,  1.3644e-01, -1.9971e-02,  5.0089e-03, -1.6958e-01,\n",
            "        -7.7552e-03, -1.3577e-01,  4.6505e-02, -5.1858e-02,  1.4171e-01,\n",
            "        -2.6246e-01,  2.2415e-02,  3.2442e-02,  2.4583e-02, -5.9259e-02,\n",
            "         9.7402e-03,  1.4199e-02, -4.5769e-02, -1.0935e-01, -2.3619e-02,\n",
            "         4.2561e-02, -6.7020e-02,  7.3488e-02, -1.7091e-01,  1.1338e-01,\n",
            "         2.4654e-01,  1.6261e-01,  1.8880e-01, -2.3696e-02, -1.1337e-01,\n",
            "         1.1181e-01, -1.0227e-01,  5.9296e-03, -3.3036e-02,  7.4877e-02,\n",
            "         6.3814e-02,  7.3918e-02, -1.0912e-01,  2.9267e-01, -1.5304e-01,\n",
            "         7.1952e-02, -4.5760e-01, -2.2703e-01,  1.6480e-02, -6.4531e-02,\n",
            "        -3.5232e-02,  2.3991e-01, -7.4406e-02, -1.3384e-01,  4.9751e-02,\n",
            "         4.8845e-02, -1.6818e-02,  1.0602e-01,  1.2001e-01, -1.7261e-01,\n",
            "        -8.2255e-02, -1.8101e-02,  3.9984e-02, -3.2806e-02,  7.9939e-02,\n",
            "        -1.0795e-02,  1.3516e-01, -2.6088e-01,  6.0838e-02, -4.7273e-02,\n",
            "         5.8114e-02, -1.8105e-01, -1.1849e-01,  1.3264e-01, -2.4014e-01,\n",
            "         1.9712e-02,  2.9831e-03, -1.4202e-01,  1.5169e-03, -8.7051e-02,\n",
            "        -1.9759e-01,  2.2026e-02, -2.1364e-01, -7.0783e-02,  1.5677e-01,\n",
            "         5.4109e-02, -6.2491e-02,  3.6037e-02, -5.0481e-02,  9.0201e-06,\n",
            "        -1.3896e-01, -1.0529e-01,  6.2360e-01, -6.4138e-02,  6.4310e-02,\n",
            "         3.3369e-02,  2.3194e-01,  5.6505e-02,  1.9101e-01, -2.4834e-01,\n",
            "        -2.1548e-01, -1.3606e-02, -1.3605e-01,  3.6598e-02,  2.3451e-02,\n",
            "         1.4160e-01, -8.9418e-02, -8.4882e-02, -5.0443e-03, -4.4276e-01,\n",
            "         8.5766e-02,  2.3036e-01,  2.8258e-01, -1.0190e-01, -5.9326e-02,\n",
            "         6.1035e-02, -2.0427e-01, -2.7145e-02, -6.1004e-02, -1.1932e-03,\n",
            "         1.6046e-01,  2.0146e-02, -2.2870e-02, -5.0166e-02,  6.3527e-03,\n",
            "        -5.4814e-02, -2.5728e-02, -5.3152e-02,  2.0404e-01,  9.3013e-02,\n",
            "        -2.4048e-02, -1.2741e-01,  4.6002e-02, -2.0652e-01,  2.0648e-03,\n",
            "         1.3714e-02, -3.2463e-02, -4.2117e-02,  6.3447e-02,  1.5722e-01,\n",
            "        -2.6845e-02,  1.3145e-01, -1.1437e-02, -1.7536e-02,  1.4366e-01,\n",
            "        -1.0091e-01,  2.1202e-02,  1.6998e-01, -9.4973e-02,  9.5032e-02,\n",
            "        -6.2212e-02,  2.1184e-02,  9.3168e-02, -9.0862e-02, -3.6259e-02,\n",
            "         2.3007e-01, -2.8989e-02, -3.4423e-02, -4.5179e-03,  3.1929e-02,\n",
            "        -3.5346e-02,  2.3451e-02, -1.2460e-01,  5.3285e-02,  1.4220e-01,\n",
            "        -5.4814e-02, -7.2896e-02,  1.0032e-01, -7.8325e-02,  5.0939e-02,\n",
            "        -1.3665e-01, -1.5636e-01, -1.3654e-02, -6.2734e-02, -4.7868e-02,\n",
            "        -3.8025e-02, -2.9119e-02,  4.4671e-03,  4.4899e-02,  3.0698e-02,\n",
            "        -1.2907e-01, -9.9164e-02,  2.1148e-01,  1.7792e-01,  1.0937e-01,\n",
            "        -1.0888e-02, -3.5178e-02, -2.3583e-01,  5.6422e-02, -2.3896e-01,\n",
            "         1.2036e-01, -8.3772e-03, -6.6789e-02,  1.5820e-01,  4.3252e-02,\n",
            "        -2.8000e-02,  7.1731e-02, -5.5210e-02,  2.6368e-02,  1.4523e-01,\n",
            "        -1.4917e-01,  1.0304e-01,  1.3115e-01, -1.0742e-02,  4.2107e-02,\n",
            "         1.2408e-01, -4.5436e-02, -1.7417e-02,  2.8590e-01,  2.7366e-01,\n",
            "         7.4568e-02, -2.2345e-01,  1.5622e-01,  1.3238e-01,  1.0704e-01,\n",
            "        -3.6760e-02, -2.0078e-02,  1.0872e-02, -8.7339e-03,  2.4780e-02,\n",
            "        -7.9681e-02,  5.6201e-02, -9.4083e-03,  1.1167e-01,  3.5533e-04,\n",
            "        -6.7986e-02, -8.7766e-02,  5.2946e-02,  3.4946e-02, -5.9661e-02,\n",
            "        -3.0496e-01, -8.7302e-02,  6.7541e-02,  3.4691e-01, -2.3115e-01,\n",
            "        -1.3326e-01, -9.0504e-02,  1.9939e-01, -2.4922e-01, -3.8671e-02,\n",
            "        -4.2099e-02,  1.6750e-01, -1.5351e-01,  6.1993e-02, -7.2963e-02,\n",
            "         5.7077e-02, -4.1410e-02, -3.0704e-01, -8.4346e-02,  5.4749e-02,\n",
            "        -1.4960e-01,  1.3193e-02,  1.3095e-01, -1.0317e-01, -3.6941e-02,\n",
            "        -2.4947e-01, -1.0898e-01,  4.9606e-02,  1.1890e-01, -2.1028e-01,\n",
            "         4.5516e-02,  6.3995e-04, -5.7812e-02, -8.7051e-02, -9.0765e-03,\n",
            "        -1.3874e-01,  1.6319e-01,  2.8887e-02, -1.7787e-02, -1.1241e-01,\n",
            "        -8.9793e-02,  1.1643e-01,  9.1009e-02, -4.4183e-02,  1.6049e-01,\n",
            "         3.8707e-02, -3.2917e-02, -8.2545e-02,  3.8974e-02,  5.2165e-04,\n",
            "        -3.0080e-01, -6.4380e-02, -1.5801e-01,  3.8514e-02,  4.2473e-02,\n",
            "         2.7199e-02, -8.3008e-02,  3.9987e-03,  6.7060e-02,  2.2157e-02,\n",
            "        -1.0638e-01,  6.5137e-02, -2.7548e-02, -1.3535e-01,  8.4129e-02,\n",
            "         4.3140e-02,  8.8673e-02, -1.4233e-02,  3.9995e-02,  2.9071e-01,\n",
            "         3.5045e-02,  9.5785e-02, -2.3479e-02,  1.6193e-02, -1.4150e-02,\n",
            "         7.1509e-02,  2.6055e-02, -5.0732e-02,  1.9261e-01,  1.2480e-01,\n",
            "        -4.0088e-02,  1.5346e-01, -1.2722e-01,  8.2517e-02,  2.7229e-02,\n",
            "         1.4232e-01, -1.0418e-01, -3.3474e-01,  1.7507e-01,  7.3629e-03,\n",
            "        -9.0467e-02, -1.6372e-01, -3.3897e-02, -1.7019e-01, -2.5146e-01,\n",
            "        -2.3024e-02,  5.1702e-02, -6.5822e-02, -6.4108e-02, -1.3819e-02,\n",
            "        -2.0056e-01, -8.7590e-02, -9.2239e-02,  1.4202e-01, -1.6182e-02,\n",
            "         5.6391e-02, -1.5704e-01, -5.8639e-02, -1.2917e-01, -4.4512e-02,\n",
            "        -2.4761e-01, -5.4522e-02,  8.7537e-02, -9.4706e-02,  5.9970e-02,\n",
            "         1.9801e-02,  5.9993e-02,  1.4758e-01,  2.3670e-02, -1.7278e-02,\n",
            "        -6.1148e-02, -1.2242e-01, -3.5868e-02,  1.2679e-01,  7.3202e-02,\n",
            "         5.9791e-02,  1.5865e-01, -4.4356e-02,  1.6237e-01, -1.3024e-01,\n",
            "        -4.7832e-02, -1.2204e-01, -1.8643e-01, -7.7179e-02, -1.7893e-01,\n",
            "        -2.3468e-02,  6.7631e-02,  8.1101e-02, -1.0369e-02,  2.4683e-01,\n",
            "        -8.5490e-02, -1.3732e-01, -5.3011e-02, -3.0138e-02,  2.4074e-02,\n",
            "        -8.7954e-03, -5.5433e-02, -1.4018e-01,  5.5885e-02, -6.9254e-02,\n",
            "        -5.9386e-02, -5.5354e-02, -2.0383e-01, -1.9581e-01,  1.2319e-01,\n",
            "        -5.9572e-02, -3.0017e-01, -4.4909e-02, -2.5463e-01,  1.8558e-01,\n",
            "        -9.6305e-02,  1.6557e-01,  8.2282e-03,  4.2863e-02, -1.4509e-02,\n",
            "        -8.5808e-02, -2.6625e-02, -9.1070e-02,  1.3796e-03,  8.0755e-03,\n",
            "         5.4520e-02, -1.1805e-01, -1.3905e-01,  6.3126e-02, -1.1646e-01,\n",
            "         1.4987e-01,  9.7673e-02, -6.3879e-02, -5.3000e-02,  5.1778e-03,\n",
            "         2.3151e-01,  3.0575e-02,  8.9226e-02,  1.5202e-01, -6.3648e-02,\n",
            "         1.1124e-02, -5.1689e-02,  6.2043e-02, -5.5330e-02,  8.7155e-02,\n",
            "         1.1878e-02, -9.9673e-02,  2.9224e-02, -1.3160e-02, -1.3763e-01,\n",
            "         1.0733e-01,  2.2109e-02,  3.9513e-02, -3.3251e-01, -8.4885e-02,\n",
            "        -6.5448e-02,  3.8489e-03, -1.5951e-02, -9.8851e-02, -7.0251e-02,\n",
            "        -7.6264e-02, -2.5555e-01, -4.0240e-02,  2.1350e-01, -1.2665e-01,\n",
            "         6.3981e-03, -9.1910e-02, -5.4963e-03,  8.7917e-02,  1.7407e-02,\n",
            "         7.5522e-02,  7.4171e-02,  1.5086e-01,  5.8942e-02, -7.1120e-02,\n",
            "        -2.3869e-01, -9.3214e-02, -3.3312e-01,  4.2759e-02,  1.2746e-01,\n",
            "         1.1794e-02,  6.3540e-02,  5.8438e-02, -1.2289e-01, -8.5457e-02,\n",
            "         1.2247e-01,  9.3570e-02,  7.8853e-02, -6.4587e-02,  2.7778e-02,\n",
            "         2.8642e-02,  5.2274e-02, -5.9621e-02])\n",
            "2. Token: '▁বাংলায়' → tensor([ 1.4793e-02,  1.2980e-01,  7.0248e-02, -2.2069e-01,  9.7072e-02,\n",
            "        -3.3474e-02,  8.7667e-02,  1.4910e-01, -1.2079e-01, -5.3274e-02,\n",
            "        -4.5046e-02, -9.0802e-03,  7.8667e-02, -1.0989e-01, -4.7313e-02,\n",
            "        -6.7984e-02, -9.4300e-02,  5.0393e-02,  1.0526e-01,  8.6796e-02,\n",
            "         1.4409e-01,  1.1110e-01, -1.0144e-01, -1.8922e-01, -1.2749e-01,\n",
            "         4.7730e-02,  5.8110e-02,  8.0933e-02, -7.1649e-03, -5.3037e-02,\n",
            "         8.3790e-02,  2.1009e-02,  1.7892e-03, -1.4418e-01,  2.2937e-02,\n",
            "         9.6823e-02, -1.2184e-01,  9.9020e-02, -4.8092e-03, -2.7492e-01,\n",
            "         2.1715e-02, -5.1166e-02,  4.2398e-02, -5.0436e-02,  1.5655e-01,\n",
            "        -6.9516e-02,  9.1502e-02, -2.6731e-02,  2.4055e-01,  1.3227e-01,\n",
            "        -1.2923e-01, -1.4435e-01, -2.8150e-03,  8.7438e-02, -3.8979e-03,\n",
            "         1.0586e-01, -1.5711e-01, -8.6288e-02, -5.5879e-03,  1.0475e-01,\n",
            "        -6.6681e-02,  1.2131e-01, -6.9727e-02,  8.2286e-02, -7.4384e-02,\n",
            "        -1.7233e-01,  1.7200e-01, -5.3754e-03,  4.7740e-02, -5.2373e-02,\n",
            "         1.2352e-01,  1.9655e-02, -1.0651e-01, -2.7978e-02,  1.3897e-01,\n",
            "        -2.0995e-01, -1.3232e-01,  7.9396e-02, -1.9338e-02,  1.4535e-03,\n",
            "         7.4887e-02, -1.2796e-01, -1.2686e-01,  2.5028e-02, -1.4651e-01,\n",
            "         1.5750e-01, -2.9808e-02, -9.2505e-02,  1.8385e-02,  1.0477e-01,\n",
            "         6.0903e-02, -1.3986e-01, -5.0245e-02,  3.6993e-04, -9.6357e-02,\n",
            "        -1.1395e-01, -1.7108e-01,  5.3848e-02,  1.7041e-02,  4.9593e-02,\n",
            "         3.4803e-02, -3.0836e-02, -1.1998e-01, -6.2432e-04,  1.1443e-01,\n",
            "        -2.0959e-02, -1.1442e-01, -4.9736e-02, -4.2267e-02,  8.3710e-03,\n",
            "        -4.7223e-02,  2.2269e-02, -7.1549e-02,  1.2468e-01,  1.1762e-01,\n",
            "         6.0828e-02,  2.7709e-02,  8.8723e-02,  1.0884e-02, -1.3893e-01,\n",
            "         5.9233e-02, -6.8048e-03, -1.3585e-01,  4.0455e-02, -9.8983e-02,\n",
            "        -5.3071e-02, -1.1947e-02, -1.9667e-01, -1.8938e-02, -8.3445e-02,\n",
            "        -2.2264e-02, -4.4514e-02,  3.8491e-03, -2.2099e-01,  2.6467e-02,\n",
            "         7.5593e-02, -8.2052e-02,  5.1340e-02,  2.4877e-01,  1.6545e-01,\n",
            "         1.1389e-01, -3.8136e-02, -1.7419e-01,  3.4854e-02,  1.5150e-01,\n",
            "         9.4392e-02, -5.8141e-02, -3.8412e-03, -1.2439e-02,  1.0674e-01,\n",
            "         4.6467e-02, -7.3244e-02, -1.6739e-01, -1.9619e-01, -7.6043e-02,\n",
            "        -2.0440e-02, -1.1269e-01, -2.6301e-02,  1.5415e-01, -1.1230e-01,\n",
            "        -1.2062e-01,  1.0524e-01, -4.2078e-02,  2.1549e-02, -2.2576e-02,\n",
            "         3.1004e-01,  7.4709e-03, -3.0017e-02, -2.8028e-02, -1.3197e-01,\n",
            "        -3.2720e-02, -1.0757e-01,  4.5476e-02, -3.6022e-02, -1.4628e-01,\n",
            "         1.7632e-01, -6.7373e-02, -1.2044e-01, -4.8107e-02,  1.8885e-01,\n",
            "         1.6447e-01, -4.7489e-02, -4.4644e-03, -4.0086e-03,  1.1884e-02,\n",
            "        -6.7893e-02, -1.1050e-01,  1.7850e-01, -8.0305e-02, -5.8915e-02,\n",
            "         1.0255e-01,  1.0391e-01,  2.4805e-02,  9.0308e-02, -8.1299e-02,\n",
            "        -3.5200e-02, -4.0353e-02, -5.7971e-03, -1.0695e-01,  3.7673e-02,\n",
            "        -1.5552e-02,  9.1924e-02, -1.2905e-01, -4.4370e-02, -1.5760e-01,\n",
            "         1.3152e-01, -2.0516e-02, -9.1471e-02,  3.4840e-02,  7.0448e-02,\n",
            "         2.8492e-02, -7.6282e-03,  6.2803e-02, -1.1526e-01, -6.6670e-02,\n",
            "        -1.0040e-01, -1.2316e-01,  1.9725e-01,  1.6180e-01, -6.2433e-02,\n",
            "         6.8307e-02, -1.4359e-01, -1.5188e-03, -4.1919e-02,  2.0454e-01,\n",
            "        -1.6247e-01, -1.3757e-01,  7.1782e-02, -1.3145e-01, -1.1535e-01,\n",
            "         6.8326e-02,  1.2777e-01, -1.1927e-01,  4.6067e-02,  2.2008e-01,\n",
            "        -6.9675e-02, -4.5743e-02,  1.7279e-02, -5.9252e-02,  1.3048e-01,\n",
            "        -2.1643e-02, -2.3993e-03, -8.3137e-02, -7.2381e-02, -4.9775e-02,\n",
            "        -3.3232e-01,  4.1583e-02, -2.8685e-02,  4.8854e-02,  4.8973e-02,\n",
            "        -5.5121e-02, -6.3848e-02, -1.6162e-01,  2.6283e-02,  1.6286e-03,\n",
            "         2.0754e-01, -3.4916e-02,  2.2168e-02, -2.7579e-01, -1.1852e-01,\n",
            "        -1.6957e-01, -2.1793e-01, -1.5180e-01, -1.9315e-02, -9.8267e-02,\n",
            "         2.2797e-02, -9.8007e-02,  5.3247e-03,  3.8159e-02, -1.5618e-01,\n",
            "         6.0802e-02,  1.6836e-01,  5.0001e-02, -1.7728e-01, -4.3990e-02,\n",
            "        -1.1625e-01,  1.2844e-02, -4.6243e-02, -4.0495e-02, -7.3182e-02,\n",
            "        -2.9846e-03, -2.5706e-02,  1.0122e-01,  1.4434e-01, -2.6175e-02,\n",
            "        -2.2790e-01,  1.7545e-02, -1.4569e-01,  1.7319e-01, -5.3516e-02,\n",
            "        -1.9996e-02,  1.0350e-01, -1.3387e-01, -4.3558e-03,  1.0817e-01,\n",
            "         9.6059e-02, -3.0582e-02, -5.0934e-02, -1.4547e-01, -1.1498e-01,\n",
            "         2.7529e-02, -5.0431e-03,  2.8973e-02,  1.0371e-01,  2.5429e-02,\n",
            "         1.1457e-01,  7.4311e-02,  2.7067e-02, -1.5795e-03, -9.7920e-02,\n",
            "         4.9268e-02,  2.3187e-04, -3.2103e-02,  3.0714e-02,  1.1315e-01,\n",
            "        -1.8671e-01, -5.7436e-02, -8.5873e-02, -5.7936e-02, -1.6300e-02,\n",
            "        -5.8826e-02,  5.1979e-02, -9.7600e-03, -1.4070e-01, -1.4166e-01,\n",
            "         1.1745e-01, -3.0596e-02, -8.2521e-02, -1.4450e-01,  1.9972e-01,\n",
            "        -6.4558e-02,  3.1509e-02,  5.8869e-02, -8.7541e-02,  1.3960e-02,\n",
            "        -6.4832e-02, -8.0526e-02,  6.2370e-02, -4.9600e-02,  6.2941e-02,\n",
            "         4.0446e-02, -9.4421e-03,  9.3424e-02,  2.2581e-01, -1.2925e-01,\n",
            "         6.4910e-03, -2.1797e-01, -1.1819e-01,  8.2496e-02, -6.5789e-02,\n",
            "         8.3098e-02,  1.6159e-01, -9.7931e-02, -1.1120e-01,  8.3338e-02,\n",
            "         8.3195e-02,  2.1476e-02, -1.0039e-01,  1.0576e-01, -3.5362e-02,\n",
            "        -8.0443e-02,  5.6646e-02,  4.0889e-02, -4.2157e-02,  2.0698e-01,\n",
            "        -1.3637e-01,  6.3201e-02, -1.9006e-01,  8.5265e-03,  2.6208e-02,\n",
            "         2.4449e-01, -1.5410e-01, -4.2611e-02,  3.3099e-02, -9.3362e-02,\n",
            "         2.1567e-02, -5.3364e-02,  6.9449e-02, -2.5872e-02, -3.2656e-02,\n",
            "        -1.0775e-01, -6.8698e-02, -2.5631e-01, -6.0438e-02,  7.2441e-02,\n",
            "        -3.2164e-02, -1.2062e-02, -5.8918e-02, -6.3086e-02, -1.5336e-06,\n",
            "        -1.3347e-01,  5.3626e-02,  6.6301e-01, -3.0009e-02, -7.4097e-02,\n",
            "         1.2566e-01,  1.6982e-01, -4.9743e-02,  1.8256e-01, -1.8970e-01,\n",
            "        -1.9192e-01,  7.5571e-02, -9.1805e-02, -1.4271e-01,  2.5070e-02,\n",
            "         5.7340e-02, -1.4615e-01, -1.3666e-02, -1.7001e-02, -3.8903e-01,\n",
            "         1.3721e-01,  2.3363e-01,  7.6014e-02, -2.6753e-01,  1.3013e-01,\n",
            "         1.8973e-01,  9.2025e-03,  1.6349e-01, -5.4571e-03, -7.8906e-02,\n",
            "         1.4597e-01, -4.8974e-02, -6.7409e-02, -1.3778e-01,  9.0632e-02,\n",
            "        -3.2364e-01, -7.5131e-02, -8.0637e-02,  2.6026e-01,  4.8068e-02,\n",
            "         1.6359e-02, -3.2259e-02, -1.2792e-02, -7.1329e-02,  2.8053e-02,\n",
            "         3.1351e-03, -1.4847e-02,  8.0075e-02,  1.7973e-01,  3.5502e-02,\n",
            "        -8.6618e-03,  2.6372e-01, -4.4227e-02, -7.2845e-03,  5.8876e-02,\n",
            "        -1.2390e-01, -1.3077e-01,  2.0408e-01, -1.5063e-01, -1.0103e-02,\n",
            "         6.3694e-02, -4.7634e-02,  5.5070e-02, -9.5970e-02,  1.6882e-01,\n",
            "         1.0074e-01,  1.9882e-02,  8.1848e-02, -9.6309e-02,  2.0196e-01,\n",
            "        -1.2213e-01, -1.4518e-02, -2.1512e-02, -1.6900e-02, -7.4134e-02,\n",
            "        -8.6429e-03, -1.2248e-01,  4.9124e-02, -3.6168e-02,  1.2135e-01,\n",
            "        -8.5001e-02,  4.4850e-02, -1.0457e-01, -6.0392e-03, -7.2302e-02,\n",
            "        -3.8985e-02, -1.0462e-01, -7.4196e-03,  1.0435e-01,  5.8134e-03,\n",
            "         9.4471e-02,  1.1115e-01,  2.1804e-01,  3.8973e-02,  1.2726e-01,\n",
            "        -4.5879e-03,  1.7174e-02, -9.6593e-02,  6.8946e-03, -1.4657e-02,\n",
            "        -1.7110e-02,  1.6099e-02, -1.1386e-01,  1.6059e-01, -1.1563e-01,\n",
            "        -1.0171e-01,  3.5071e-02, -3.9844e-02,  1.8336e-01,  6.5904e-02,\n",
            "        -1.7624e-01, -2.6754e-02,  7.6659e-02, -8.4164e-02,  8.4117e-02,\n",
            "         1.0797e-01,  2.3949e-02, -1.0544e-01,  2.1725e-01,  2.0449e-01,\n",
            "        -2.4403e-02,  1.6552e-02,  1.2243e-01,  2.3834e-01,  6.1418e-02,\n",
            "        -1.8962e-01,  5.4455e-03,  1.2319e-01, -6.1141e-02, -1.8748e-01,\n",
            "        -1.3715e-01, -1.2584e-02,  5.1974e-02,  1.4437e-01, -1.9465e-05,\n",
            "        -1.4634e-01, -1.2620e-02,  3.9639e-02,  1.1224e-01,  1.2968e-02,\n",
            "         6.2189e-03,  3.0650e-03, -1.0491e-01,  8.8933e-02, -3.3555e-02,\n",
            "         2.4805e-02, -1.1242e-01,  8.9165e-02, -1.1288e-01, -1.8698e-01,\n",
            "        -1.5652e-02,  3.0734e-02, -1.7622e-01, -3.7434e-02, -2.2159e-01,\n",
            "         2.2233e-01, -8.4437e-02, -1.8179e-01, -2.0002e-01, -6.8513e-02,\n",
            "         3.2434e-02, -7.4921e-02, -2.1817e-02, -8.7302e-02, -7.1884e-02,\n",
            "        -2.2936e-01, -6.9009e-02, -5.8355e-02,  4.8623e-02, -8.0805e-02,\n",
            "        -2.9629e-03,  5.3698e-04, -3.5584e-02, -8.4438e-02,  6.3867e-02,\n",
            "         6.1235e-03,  2.1705e-01, -4.0947e-02,  9.1586e-02,  9.7167e-02,\n",
            "         5.3013e-02,  7.8756e-02, -8.5793e-02, -4.8488e-02,  1.0651e-01,\n",
            "        -5.6357e-02, -1.4172e-02, -9.1640e-02,  6.2959e-02,  7.3668e-04,\n",
            "         2.3697e-01,  3.1901e-02, -1.4462e-01,  4.4499e-02, -4.5784e-02,\n",
            "        -5.8158e-02, -4.3882e-02, -2.2138e-02,  9.1894e-03,  2.4444e-02,\n",
            "        -2.5605e-01,  1.3331e-01, -8.9680e-02, -3.4618e-02, -5.9818e-02,\n",
            "        -2.1293e-02,  2.2158e-02, -6.3268e-03, -1.6704e-01,  1.8977e-01,\n",
            "        -6.3136e-02, -6.5801e-02,  8.1129e-02, -5.6409e-02, -1.2439e-01,\n",
            "        -1.1564e-02,  1.7112e-01, -4.3556e-02, -1.9097e-03, -3.7782e-03,\n",
            "        -5.5437e-03,  1.6299e-01, -1.2829e-01, -3.1800e-02,  9.1194e-02,\n",
            "         1.1539e-01, -1.0112e-01, -3.1247e-01,  1.4291e-01,  1.7919e-01,\n",
            "        -1.7175e-01, -1.8011e-01,  8.2291e-02,  5.2070e-02, -1.9426e-01,\n",
            "        -1.6470e-02,  2.2885e-01,  7.8683e-02, -6.6805e-02, -1.2406e-01,\n",
            "        -1.1882e-01, -3.6461e-02, -2.9356e-03,  1.6082e-01, -1.5637e-01,\n",
            "         9.4846e-04, -1.7075e-01,  4.7261e-03, -1.7248e-01,  6.1864e-02,\n",
            "        -9.8081e-02,  9.2616e-02, -2.1363e-02, -1.4354e-01,  1.7107e-01,\n",
            "        -7.7751e-02,  6.4478e-02,  1.1709e-01,  2.4028e-01,  9.0416e-02,\n",
            "        -2.5688e-02, -1.2351e-01,  5.2599e-02,  2.4779e-01, -1.5901e-01,\n",
            "        -9.5468e-02,  5.6064e-02, -1.9960e-01,  2.2233e-01, -7.1132e-02,\n",
            "        -1.7584e-01, -2.0195e-01, -1.2003e-01, -6.5239e-02, -1.1148e-01,\n",
            "        -3.6252e-03,  9.9811e-02,  2.0390e-01,  2.3640e-02,  1.1218e-01,\n",
            "        -1.0029e-02, -9.1696e-02, -1.3625e-02,  1.5342e-02, -1.0001e-02,\n",
            "         1.2320e-02,  7.8232e-02, -1.1988e-02,  8.6565e-02, -1.3708e-03,\n",
            "        -3.8045e-02, -1.2733e-01,  2.7979e-02, -2.2498e-01, -6.0734e-02,\n",
            "         1.0387e-03, -2.7653e-01, -1.0175e-01, -1.6506e-01,  5.9363e-02,\n",
            "        -1.4530e-01,  6.1783e-02, -5.0425e-02,  1.1176e-01, -1.0895e-01,\n",
            "         1.6525e-01, -7.4456e-03, -1.6337e-01,  1.8949e-03, -5.0053e-02,\n",
            "        -4.2237e-02, -5.0827e-02, -1.2307e-01,  4.2597e-02,  2.7144e-02,\n",
            "         7.9837e-02, -6.5238e-03,  6.0180e-02, -7.1041e-02, -8.6737e-02,\n",
            "         2.1394e-01,  9.1183e-02,  9.7006e-02, -4.4855e-04, -2.9161e-02,\n",
            "        -1.6123e-02, -1.4612e-01, -8.4591e-03,  3.2734e-02, -5.6949e-02,\n",
            "        -1.3348e-01,  7.4581e-02, -5.6263e-02, -4.6043e-02,  5.7844e-03,\n",
            "         2.9464e-02, -3.8592e-02,  2.4642e-03, -1.4803e-01, -7.2175e-03,\n",
            "         8.9211e-02, -1.0894e-02, -9.5810e-02,  4.6261e-02, -9.4625e-02,\n",
            "         3.0002e-02, -1.4593e-01, -4.2835e-02,  1.6418e-01, -7.5969e-02,\n",
            "        -2.2406e-02,  2.8239e-03, -5.0466e-02,  5.2105e-02,  2.5466e-02,\n",
            "         2.3457e-01,  9.3622e-02,  1.5219e-01,  2.0265e-01, -5.0890e-02,\n",
            "        -1.5474e-01, -1.0409e-01, -1.5373e-01,  4.9078e-02, -1.4805e-01,\n",
            "        -1.2086e-01,  1.1035e-01,  1.3720e-01, -1.8774e-01, -1.7810e-02,\n",
            "        -3.0643e-01,  3.7027e-02,  5.6437e-02, -6.5934e-02,  3.3690e-02,\n",
            "        -1.2748e-01,  4.5080e-02,  1.0871e-02])\n",
            "3. Token: '▁কথা' → tensor([-2.2044e-02,  3.9189e-02,  6.0785e-02, -2.1629e-01,  1.1256e-01,\n",
            "         4.1569e-02,  3.1591e-02,  8.2006e-02, -6.7608e-02,  3.7413e-02,\n",
            "         4.7328e-02, -8.4349e-03,  1.0032e-01,  9.8150e-03, -2.2136e-01,\n",
            "         5.6781e-02,  3.2606e-02, -7.4769e-03, -3.1433e-02,  1.3956e-01,\n",
            "         6.4418e-02,  2.4000e-01, -5.9798e-02, -6.1862e-02, -1.7644e-01,\n",
            "        -5.2421e-03, -1.0240e-01, -7.3410e-02,  4.1824e-02, -5.6301e-02,\n",
            "        -8.0880e-02, -1.1339e-01,  4.6231e-02,  1.1612e-02, -7.6700e-02,\n",
            "         9.3336e-02, -1.8091e-01,  1.7083e-01,  6.0545e-02, -2.1168e-01,\n",
            "         8.2052e-02, -7.6679e-02,  9.3019e-02, -1.1967e-01,  2.8962e-01,\n",
            "        -7.7391e-02,  8.7397e-02, -1.4925e-03,  3.7277e-03,  8.7974e-02,\n",
            "         9.1760e-02, -1.6323e-01, -3.3875e-02,  9.7472e-02,  7.4700e-02,\n",
            "         1.5059e-02, -3.3813e-03, -9.5665e-02,  1.4855e-03, -6.6345e-02,\n",
            "        -1.3547e-01, -7.8852e-02, -9.7739e-02, -6.7291e-02, -1.0498e-01,\n",
            "        -1.1239e-01,  1.3691e-01, -4.0248e-02, -1.0035e-02, -4.0904e-02,\n",
            "         2.1175e-01,  3.7810e-02, -9.1239e-02, -6.0747e-03,  1.0675e-01,\n",
            "        -4.7111e-02, -8.1030e-02, -2.9989e-02, -3.9638e-02,  3.2864e-02,\n",
            "        -1.1449e-01, -5.5971e-02, -7.9063e-02, -7.6514e-03, -2.1007e-01,\n",
            "         9.0043e-02, -1.7077e-01, -2.5888e-02, -1.3252e-01,  1.3548e-01,\n",
            "         1.2988e-01,  2.6872e-02,  2.1781e-02,  9.9691e-05, -5.7351e-02,\n",
            "        -8.0942e-02, -2.8454e-01,  8.2883e-02, -4.3690e-02,  4.0427e-02,\n",
            "        -1.3026e-02, -8.3181e-02, -2.3836e-01, -5.4206e-04,  9.7968e-02,\n",
            "        -5.9323e-02, -3.2293e-02, -1.4512e-01, -3.0499e-02,  1.1166e-01,\n",
            "         1.4940e-01,  9.3421e-02, -1.8700e-01,  1.4245e-01,  6.4320e-02,\n",
            "         1.4658e-02,  3.3905e-02, -3.4953e-02, -4.0186e-02,  6.5540e-02,\n",
            "         6.0749e-02, -4.4585e-02, -1.6111e-02, -1.2981e-02,  1.4384e-01,\n",
            "        -4.8153e-02, -3.4847e-02, -1.2326e-01, -2.2218e-02, -1.4053e-02,\n",
            "        -2.9075e-02, -6.9644e-02,  7.8407e-02, -8.4247e-02,  1.2905e-01,\n",
            "        -1.2601e-01, -6.2788e-02, -2.3990e-02,  2.7501e-01,  1.1175e-01,\n",
            "         5.0412e-02, -1.1013e-01, -1.3119e-01,  1.6633e-01,  1.2827e-02,\n",
            "        -2.6796e-02, -4.0390e-02, -6.3648e-02, -2.5439e-02,  4.5080e-02,\n",
            "         8.2539e-02, -7.1096e-02, -1.8912e-01, -1.6805e-01,  5.0808e-02,\n",
            "        -2.4988e-02, -5.2517e-02,  1.1076e-01,  2.1261e-01, -5.9312e-02,\n",
            "        -5.5111e-02, -2.6566e-02, -2.0552e-01,  4.7322e-02, -2.9129e-02,\n",
            "         4.3821e-01,  2.3223e-03, -8.6990e-03,  7.9221e-02, -2.0192e-02,\n",
            "        -1.7916e-01, -5.6217e-02,  1.2942e-01, -1.0650e-02, -1.6563e-01,\n",
            "         1.1196e-01, -5.7267e-02, -8.8421e-02,  2.1645e-02, -4.7287e-02,\n",
            "         1.3314e-01,  1.5467e-04,  1.0819e-01, -1.7242e-01, -1.1957e-02,\n",
            "        -8.0114e-02,  5.9170e-03,  1.1282e-02, -7.8227e-02, -3.9949e-02,\n",
            "         9.2856e-02,  7.9340e-02,  9.3612e-02,  5.3672e-03, -1.9500e-01,\n",
            "         2.5971e-03,  4.6470e-02, -1.3034e-01, -1.3884e-02,  1.1733e-01,\n",
            "        -1.4489e-01, -1.0713e-01, -1.1789e-01, -1.0762e-01, -1.9098e-01,\n",
            "         6.5412e-02, -4.7965e-03,  1.5190e-02,  2.0110e-02, -8.8270e-02,\n",
            "         6.9535e-02, -3.6638e-01,  1.1995e-01, -1.0054e-01, -5.1696e-02,\n",
            "         1.6583e-01,  1.7144e-02,  1.8762e-01,  6.2054e-02, -7.0115e-02,\n",
            "        -2.5062e-02, -2.1543e-02, -5.3660e-02, -1.5061e-01,  3.5813e-02,\n",
            "        -8.6733e-02,  5.8938e-02, -2.1154e-02, -2.1187e-02, -5.3767e-02,\n",
            "         6.7388e-02,  2.0835e-01,  6.6729e-02,  3.5224e-03, -7.3136e-03,\n",
            "        -1.2223e-01, -1.6932e-02, -8.7682e-02,  3.7003e-02, -1.1267e-02,\n",
            "         3.2944e-02,  5.5148e-02, -2.2858e-02, -9.1998e-02, -1.7734e-01,\n",
            "        -2.0083e-01, -4.6876e-02,  2.9652e-02,  6.7317e-02,  4.3999e-02,\n",
            "        -5.5496e-02, -1.5602e-01, -1.1888e-01,  1.3473e-01, -9.2414e-02,\n",
            "         1.6637e-01,  4.6329e-03, -1.0825e-02,  6.1803e-02, -2.0712e-01,\n",
            "        -4.2667e-02, -1.8596e-01, -4.2132e-02,  7.1750e-02, -1.7179e-03,\n",
            "        -4.8603e-02, -3.1384e-02,  3.0417e-02,  9.3245e-02, -1.0111e-01,\n",
            "        -3.2700e-02,  1.0090e-01, -4.6664e-02, -4.0806e-02, -1.8553e-01,\n",
            "        -7.1547e-02,  3.3066e-02, -1.8922e-01,  9.6276e-02, -2.0145e-02,\n",
            "         8.2125e-02, -1.3474e-01,  6.4751e-02, -2.5669e-02,  6.2933e-02,\n",
            "        -1.9497e-01,  7.4853e-02, -1.1259e-01,  1.4318e-01,  4.5524e-02,\n",
            "         5.5807e-03,  3.3473e-02, -4.1331e-03, -1.3359e-01,  7.7851e-02,\n",
            "         1.5532e-01, -8.1089e-02, -1.2571e-01, -1.5960e-01, -7.8545e-02,\n",
            "         5.9501e-02,  3.6259e-02, -3.0361e-02,  5.6691e-02,  3.2480e-02,\n",
            "         1.2028e-01,  1.1720e-01,  2.4734e-02, -2.0988e-01, -1.1847e-02,\n",
            "         2.4897e-02, -2.0125e-01, -4.4711e-02, -9.2516e-02,  7.8564e-02,\n",
            "        -9.0345e-02,  5.0342e-02, -1.2730e-01, -1.4366e-02,  1.2571e-02,\n",
            "         1.7492e-02,  7.3103e-02,  9.1668e-02, -1.1550e-01,  4.2262e-02,\n",
            "         1.8532e-02,  7.3786e-02, -1.0156e-02,  2.1072e-02, -1.7846e-02,\n",
            "         1.4369e-01,  1.3877e-01,  1.1551e-01, -9.2219e-02,  1.7918e-02,\n",
            "         6.1658e-02, -2.3476e-01,  9.6193e-02, -8.0348e-02, -3.0640e-02,\n",
            "         1.1024e-01, -1.0726e-02, -9.1968e-02,  2.3980e-01, -6.0416e-02,\n",
            "         7.5070e-02, -2.9376e-01, -3.9908e-02,  4.5023e-02, -1.3803e-01,\n",
            "         3.1482e-03,  1.3258e-01, -1.4251e-01,  1.1528e-01,  1.0015e-01,\n",
            "        -8.5290e-02, -1.8010e-02,  3.7422e-02,  1.8662e-01, -1.8315e-01,\n",
            "        -2.0695e-01, -2.7779e-02,  7.2319e-02, -1.6900e-01,  2.3212e-01,\n",
            "        -1.3784e-01, -2.1858e-02, -1.8501e-01,  1.3840e-01,  1.9016e-02,\n",
            "         9.3890e-02, -1.3293e-01, -1.9856e-01,  1.7545e-01, -2.0112e-01,\n",
            "         1.2217e-01, -4.2686e-02, -2.1192e-02,  3.6849e-02, -1.9729e-01,\n",
            "        -1.7004e-01, -5.8443e-02, -2.1799e-01,  6.7311e-02, -2.4683e-02,\n",
            "         7.7381e-02, -9.1580e-02,  5.6698e-03, -1.8192e-01, -8.6667e-07,\n",
            "        -1.3596e-01,  1.2572e-01,  5.8892e-01, -6.4851e-02, -2.7590e-03,\n",
            "         9.2925e-02,  1.6809e-02, -1.1157e-01,  1.8855e-01, -9.1624e-02,\n",
            "        -8.4518e-02,  5.7837e-02, -1.2336e-01, -1.5122e-02,  7.6661e-02,\n",
            "         1.1730e-01, -2.4158e-02, -2.3978e-02,  1.6930e-02, -5.8515e-01,\n",
            "         3.6414e-02,  9.4810e-02,  1.2642e-01, -1.8314e-01,  9.2262e-02,\n",
            "         5.1515e-02, -1.9569e-01,  1.5811e-01,  1.2798e-02,  8.1900e-03,\n",
            "         1.6189e-01,  1.7825e-01, -3.6475e-02, -4.3954e-02,  4.9964e-02,\n",
            "        -5.3594e-02,  6.7209e-02,  4.8618e-02,  1.8410e-01,  6.6476e-02,\n",
            "         3.3761e-02, -1.3483e-01,  2.5493e-02, -1.4362e-01,  1.4985e-02,\n",
            "         8.8587e-02,  8.1540e-02,  1.1097e-01,  7.9722e-02,  1.1372e-01,\n",
            "         8.1575e-03,  4.0538e-02,  5.1233e-02,  7.9419e-02,  2.8799e-01,\n",
            "        -6.3246e-02, -1.2430e-01,  2.5513e-01, -8.1246e-02,  1.6120e-01,\n",
            "         8.1122e-02, -1.1612e-01,  7.9915e-03,  5.8300e-02,  1.9494e-02,\n",
            "         1.5873e-01,  1.1018e-01, -2.0321e-02, -4.5421e-02,  1.8588e-01,\n",
            "        -5.6038e-02, -8.4068e-02, -1.8023e-02,  6.0596e-02,  4.2918e-04,\n",
            "         1.0800e-01, -1.5720e-01,  3.0190e-01,  2.5344e-03,  2.4560e-02,\n",
            "        -1.9437e-01, -3.7577e-02, -2.0092e-01, -1.2805e-02, -2.5172e-02,\n",
            "         6.4112e-03, -1.3818e-01, -6.9190e-03,  3.9615e-02,  1.3127e-01,\n",
            "        -1.6873e-03, -2.6877e-02,  2.1980e-01,  6.0843e-02,  6.0564e-03,\n",
            "        -1.8240e-02,  4.3905e-04, -1.8172e-02, -6.1289e-02, -8.0255e-02,\n",
            "         8.9800e-02,  4.1685e-02,  5.4734e-02,  1.5075e-01,  2.6958e-01,\n",
            "        -6.9503e-02,  1.5561e-01, -1.5004e-01, -9.4630e-03,  1.9985e-01,\n",
            "        -1.3309e-01,  8.8461e-03,  9.7743e-02,  1.2274e-01,  4.7059e-02,\n",
            "        -7.6649e-02, -2.3320e-02, -7.3310e-02,  1.9879e-01,  4.4097e-01,\n",
            "         1.0571e-01,  2.8233e-02,  2.1741e-01,  2.3575e-01,  4.0129e-02,\n",
            "        -3.7081e-02,  8.7129e-02,  9.2665e-02, -1.2777e-01, -1.0415e-01,\n",
            "        -5.6335e-02,  1.0138e-01, -2.4228e-02,  2.8071e-03,  3.3688e-04,\n",
            "        -5.9062e-02, -1.4305e-02,  1.7741e-01,  1.4429e-01, -1.3390e-01,\n",
            "        -3.9733e-02, -3.3966e-02,  1.2312e-01,  9.6950e-02, -4.5366e-02,\n",
            "         4.6304e-02, -2.2100e-01,  8.1159e-02, -2.0217e-01, -8.3583e-02,\n",
            "        -2.4150e-02,  8.8962e-02, -1.2452e-01, -9.6480e-02,  7.4504e-02,\n",
            "         1.5859e-01, -6.4662e-02, -3.3265e-01, -8.5908e-02, -4.1080e-02,\n",
            "         3.0180e-02, -2.2764e-02,  4.3428e-02, -1.5773e-01,  3.9813e-02,\n",
            "        -1.3149e-01, -1.0696e-02, -5.8276e-03, -2.3804e-02, -1.0214e-01,\n",
            "        -2.2790e-02,  2.7440e-04, -2.7885e-02, -1.8182e-01, -1.8120e-02,\n",
            "        -1.9543e-01,  1.0896e-01, -1.6724e-01,  5.5723e-02,  5.4420e-02,\n",
            "        -6.5132e-02,  2.3613e-01, -4.9701e-02, -1.1949e-01,  1.9929e-01,\n",
            "        -1.4957e-03, -4.3181e-03, -1.8114e-01,  7.2414e-02,  6.6778e-04,\n",
            "        -1.4211e-02, -7.2778e-02, -1.1250e-01, -1.2944e-01, -2.9443e-02,\n",
            "        -3.0510e-02, -3.7779e-02, -1.4526e-01, -1.6197e-03, -1.0292e-02,\n",
            "        -9.6322e-02,  3.0364e-02,  5.8623e-02, -1.7261e-01, -1.3268e-02,\n",
            "        -6.6416e-02,  1.9965e-02, -3.2966e-02, -1.7162e-01,  2.7331e-01,\n",
            "         1.5335e-02, -7.4687e-02,  7.4171e-02,  7.9543e-02, -4.8180e-02,\n",
            "         5.1487e-02,  9.0728e-02,  1.0360e-04,  8.7602e-02, -4.9523e-02,\n",
            "        -2.1476e-02,  5.4902e-02, -1.1081e-01,  9.6079e-02, -8.9138e-02,\n",
            "         2.8242e-01,  2.0275e-02, -2.9525e-01,  1.3096e-01,  1.2266e-01,\n",
            "        -1.4677e-01, -1.5030e-01, -1.2120e-02,  1.9544e-01, -2.0752e-01,\n",
            "         1.2230e-02,  5.3047e-02,  1.4776e-02, -1.7035e-01, -1.6808e-02,\n",
            "        -1.1848e-01, -8.7113e-02, -7.0711e-02,  1.3601e-01, -8.9537e-02,\n",
            "         3.5010e-02, -2.1114e-01, -2.0251e-02, -4.7620e-02,  9.1703e-03,\n",
            "        -2.1231e-01, -3.3629e-02,  1.1475e-01, -2.5473e-01, -2.6939e-02,\n",
            "        -2.1224e-01,  1.1477e-01,  1.9251e-01,  2.3564e-01, -2.9648e-03,\n",
            "        -1.1066e-01, -8.4416e-02,  1.3384e-02,  1.8422e-01,  3.7656e-02,\n",
            "         2.1869e-01, -2.5763e-02, -1.0882e-01,  1.9661e-01, -8.4794e-02,\n",
            "        -9.6593e-02, -2.9413e-01,  2.5032e-02, -1.2425e-04, -1.8766e-01,\n",
            "         9.4806e-03,  1.2513e-01,  2.9583e-02, -4.7943e-02, -1.6041e-02,\n",
            "        -3.4054e-02, -1.4316e-01, -4.7509e-02,  1.7383e-01, -2.2671e-02,\n",
            "         8.9490e-02, -5.6332e-02, -9.2596e-02,  7.9371e-02, -3.2802e-02,\n",
            "         8.2018e-02, -9.8975e-02, -1.9125e-01, -2.2539e-01,  1.8986e-01,\n",
            "         1.5113e-01, -3.3011e-01, -9.2708e-02, -2.9509e-01,  1.0748e-01,\n",
            "        -7.1425e-02,  3.0006e-02,  8.4126e-02,  1.1019e-01, -3.1245e-02,\n",
            "         6.3826e-02, -5.7292e-03, -1.4018e-01,  2.1720e-03,  1.1043e-01,\n",
            "         3.5290e-02, -2.0307e-01, -1.8704e-01, -3.1409e-02,  5.8825e-02,\n",
            "         5.7965e-02,  1.1330e-01, -4.0689e-02, -1.6637e-02, -3.1505e-02,\n",
            "         1.4619e-01, -1.6186e-02,  1.1304e-01,  3.3341e-02, -1.9438e-02,\n",
            "        -2.1617e-02, -1.0969e-01,  1.2925e-01,  7.7794e-02, -1.3884e-01,\n",
            "        -8.6747e-02,  1.1145e-02,  8.2249e-03,  8.3737e-02, -1.0686e-01,\n",
            "         1.4526e-01,  2.4565e-02, -4.4274e-02, -1.5947e-01,  5.0985e-02,\n",
            "         1.0975e-01, -5.8580e-02,  2.1354e-02, -8.8725e-02, -6.6405e-02,\n",
            "         1.1205e-01, -2.0173e-01,  2.7442e-02,  7.8570e-02, -3.7790e-02,\n",
            "        -1.2749e-01, -8.5233e-02, -1.6767e-03,  9.4478e-02,  1.0622e-01,\n",
            "         2.4547e-01,  3.9341e-02,  1.0869e-01,  2.4409e-01,  1.7217e-01,\n",
            "        -1.6272e-01, -5.8557e-02, -2.1994e-01, -7.0695e-02, -9.0373e-02,\n",
            "        -1.1010e-02,  2.0852e-01,  9.9088e-02, -2.2214e-01, -5.3614e-02,\n",
            "         3.0757e-02,  1.1844e-01, -5.3656e-02, -1.5720e-01,  5.2842e-02,\n",
            "         5.7799e-02,  1.3219e-01, -1.4035e-01])\n",
            "4. Token: '▁বলি' → tensor([-9.2768e-02,  1.8164e-02, -5.3722e-04, -4.9380e-02,  1.3912e-01,\n",
            "        -5.3468e-02, -5.2228e-04,  1.0241e-01, -3.0149e-02,  8.5301e-02,\n",
            "        -1.2894e-02, -9.5967e-02, -1.7369e-02, -7.8469e-02, -1.5751e-01,\n",
            "         4.3702e-02, -1.2033e-02, -2.6330e-02,  3.6844e-02,  1.3090e-01,\n",
            "         1.1054e-01,  1.8686e-01, -8.3613e-02, -1.4873e-01, -8.4772e-02,\n",
            "        -5.7967e-02, -1.1236e-01, -1.6202e-01,  5.9269e-02, -8.9614e-02,\n",
            "        -1.4311e-01, -2.5099e-01,  3.6928e-02,  6.2608e-02,  1.6982e-02,\n",
            "         1.0715e-01, -4.3193e-02,  4.9690e-02, -1.3292e-02, -1.0240e-01,\n",
            "         1.1357e-01, -6.9957e-02,  1.2115e-01, -7.1793e-02,  1.6293e-01,\n",
            "        -1.3517e-01,  5.9999e-02,  9.9249e-03,  1.4076e-01, -7.8638e-02,\n",
            "         9.7173e-02, -1.1929e-01, -1.1408e-01,  7.6589e-02,  6.9922e-02,\n",
            "         4.6176e-02,  9.6368e-02,  7.1441e-03,  3.7196e-02,  7.6609e-02,\n",
            "        -6.4424e-02, -3.5535e-02,  8.3053e-02, -5.6830e-02, -6.3421e-02,\n",
            "        -1.5149e-01,  5.7931e-02, -8.2278e-02, -9.4198e-03, -1.4008e-02,\n",
            "         2.3879e-01,  8.6972e-02,  4.9985e-02,  9.9585e-02,  1.5964e-01,\n",
            "        -4.1524e-02, -7.8855e-02, -1.2534e-01, -4.0735e-02,  1.1084e-01,\n",
            "        -7.0789e-02, -6.9087e-02, -4.7380e-02, -3.2709e-02, -1.5815e-01,\n",
            "         3.2232e-02, -2.2038e-01, -2.9514e-02,  1.4024e-03, -3.0363e-02,\n",
            "         1.0248e-01, -4.9739e-02, -1.7221e-03,  2.3952e-04,  5.6816e-02,\n",
            "        -1.6752e-01, -3.4293e-01,  1.9024e-01, -7.6965e-02,  1.2224e-01,\n",
            "         9.1374e-02, -4.9947e-02, -2.4383e-01, -4.3968e-04, -3.9553e-02,\n",
            "        -3.5159e-02,  2.2231e-02, -1.8413e-01, -8.3954e-02,  1.3592e-01,\n",
            "         7.5738e-02,  6.6595e-02,  2.4658e-02,  1.1203e-01, -1.3936e-02,\n",
            "         8.0511e-02, -4.6423e-02,  2.0075e-02,  8.4878e-03,  5.2933e-02,\n",
            "         1.3592e-01, -1.0773e-01, -7.2474e-02,  1.1612e-01,  2.3751e-01,\n",
            "        -7.5135e-02, -1.9405e-02, -8.3130e-02, -3.2395e-02, -8.4089e-02,\n",
            "         8.7425e-03, -1.8329e-02,  1.3724e-01, -1.2219e-01, -3.6438e-02,\n",
            "        -1.3545e-01,  7.6707e-04, -1.3750e-02,  1.5683e-01,  4.0059e-02,\n",
            "        -6.7606e-03, -1.2172e-01, -1.3429e-01,  1.6324e-01, -3.1900e-03,\n",
            "         1.0326e-01, -4.2419e-02,  4.5085e-02,  5.6040e-03,  7.3474e-02,\n",
            "        -1.1509e-02, -1.2188e-02, -1.2456e-01, -1.2095e-01, -1.5621e-01,\n",
            "        -1.2927e-01, -4.9449e-02,  6.7140e-02,  1.1089e-01, -9.8222e-02,\n",
            "        -5.5200e-02, -8.8276e-02, -1.5019e-01,  2.5670e-03, -1.5927e-01,\n",
            "         4.8023e-01, -3.1527e-03,  4.1651e-02, -4.1966e-02, -5.5845e-02,\n",
            "        -9.0706e-02, -1.6013e-01,  7.2310e-02, -8.9536e-03, -7.4901e-02,\n",
            "         3.8151e-02,  6.1142e-02, -2.8756e-01, -1.3612e-01,  8.3214e-02,\n",
            "         1.4147e-01, -5.3410e-02,  5.9395e-02, -4.7976e-02, -3.0060e-02,\n",
            "        -3.5423e-02, -3.9594e-02,  2.9446e-02, -1.5080e-01, -1.8590e-02,\n",
            "         1.0746e-01,  1.1277e-02,  1.3972e-01, -2.8154e-03, -7.7544e-02,\n",
            "         7.6893e-02, -3.6665e-02, -4.1545e-02,  3.7393e-02,  6.8787e-02,\n",
            "        -3.1071e-02, -4.0111e-02, -1.3419e-01, -5.8489e-02, -7.2425e-02,\n",
            "         3.2228e-02,  9.0100e-02, -1.4600e-02, -1.7040e-01, -1.7045e-02,\n",
            "         3.2256e-02, -1.4688e-02,  7.7190e-02, -9.2173e-02, -4.7916e-02,\n",
            "         2.8845e-01,  3.8454e-02, -3.0834e-02, -2.1865e-04,  2.7008e-02,\n",
            "         1.6193e-02, -1.0680e-01,  3.2871e-02, -2.5038e-02, -6.0662e-02,\n",
            "         4.9132e-02, -2.3594e-03,  1.4524e-01,  7.0598e-02,  4.8744e-02,\n",
            "         6.3251e-02,  1.4248e-01, -3.2865e-02,  1.6899e-01, -1.2763e-01,\n",
            "        -2.4397e-03,  6.6400e-02,  1.4092e-02,  2.2399e-02,  1.3525e-04,\n",
            "         8.4310e-02,  2.4402e-03,  4.9135e-02, -6.5871e-02, -1.6445e-01,\n",
            "        -2.0173e-01,  1.1960e-01, -6.3942e-02, -2.1073e-02,  4.8743e-02,\n",
            "        -9.1847e-02, -7.4179e-02, -1.0346e-01, -3.0338e-02, -1.5382e-01,\n",
            "         4.1932e-02,  6.1308e-02, -5.1717e-02,  3.1235e-02, -1.8632e-01,\n",
            "         4.1350e-02, -1.9277e-01,  2.6061e-02,  1.2161e-01,  2.7890e-02,\n",
            "         1.0124e-01, -2.8824e-01,  3.5326e-02,  3.9470e-02, -1.2734e-01,\n",
            "        -6.9290e-02,  1.5983e-01, -2.4604e-02, -1.1897e-01, -3.0653e-01,\n",
            "        -1.0391e-02, -3.1877e-03, -2.3346e-01,  3.2274e-02, -8.6890e-02,\n",
            "         8.6953e-03, -3.3153e-02,  1.8538e-01,  2.8794e-03,  8.0751e-02,\n",
            "        -1.2231e-01,  6.0240e-02, -7.2517e-02,  1.9797e-01, -9.2422e-03,\n",
            "         3.6246e-02,  3.6385e-02, -1.1812e-02, -1.8866e-01,  1.5658e-01,\n",
            "         1.3498e-01, -2.2052e-02, -4.0007e-02, -4.1318e-02, -9.4149e-02,\n",
            "        -4.2450e-02,  1.4688e-02, -8.2345e-02, -3.7437e-02,  9.3811e-02,\n",
            "         6.5865e-02,  1.0472e-01, -5.8997e-02, -4.5300e-02,  4.7848e-02,\n",
            "         2.3211e-02,  2.9897e-02,  4.0539e-03, -7.6007e-02,  8.5172e-02,\n",
            "        -1.6041e-01,  9.9366e-02, -9.1206e-03, -1.0480e-01,  2.8731e-02,\n",
            "         4.2797e-02,  9.9921e-02,  5.7775e-02, -5.3505e-02,  1.0645e-01,\n",
            "        -4.5841e-02,  5.6944e-02,  1.5825e-02, -6.9804e-02,  8.2954e-02,\n",
            "         1.3884e-01, -1.8958e-02,  3.1310e-02, -8.1208e-03, -8.1800e-02,\n",
            "        -3.6025e-03, -6.4376e-02, -4.3861e-02, -8.1575e-02, -7.8458e-03,\n",
            "         1.1145e-01,  3.0464e-02, -8.4549e-02,  3.0360e-01, -1.7789e-01,\n",
            "         1.9507e-02, -3.1074e-01, -3.8149e-02,  7.7583e-02, -1.2423e-01,\n",
            "        -3.7998e-02,  4.7022e-02, -1.3713e-01,  1.0685e-01,  9.9846e-03,\n",
            "         1.1440e-01, -3.3080e-02, -1.5791e-02,  1.6323e-01, -1.8933e-01,\n",
            "        -1.2559e-01, -2.3804e-02,  6.7498e-02, -6.3925e-02,  2.5670e-01,\n",
            "        -1.6941e-01, -7.3042e-02, -8.2179e-02,  1.6557e-03,  2.9230e-02,\n",
            "         4.0523e-02, -1.7758e-01, -1.5477e-01,  1.3775e-01, -1.9966e-01,\n",
            "         6.5626e-02, -4.6246e-02, -1.8956e-02, -5.1252e-02, -2.2633e-01,\n",
            "        -1.6089e-01, -1.4058e-01, -1.4047e-01, -4.2879e-03,  2.2611e-02,\n",
            "         7.8439e-02, -1.4620e-01, -8.7218e-02, -4.1385e-02, -7.6420e-06,\n",
            "        -8.9474e-02, -4.8889e-03,  4.7468e-01, -6.7198e-02, -9.6635e-02,\n",
            "         8.7906e-02,  1.8709e-01, -5.0006e-03,  2.1321e-01, -1.0695e-01,\n",
            "        -1.6940e-01,  1.7941e-02, -1.5891e-01, -3.8578e-02, -7.3359e-02,\n",
            "         1.6766e-01, -5.2228e-02, -2.9312e-03, -5.3949e-02, -6.0969e-01,\n",
            "        -2.6712e-02,  8.7707e-02,  1.9768e-02, -1.8493e-01, -4.8893e-02,\n",
            "         6.3558e-02, -1.7346e-01,  1.8567e-03, -1.3354e-02,  6.0263e-02,\n",
            "         8.3205e-02,  1.7616e-01, -4.3852e-02, -7.7049e-02,  9.5347e-02,\n",
            "         8.8949e-02, -4.2041e-03,  7.0313e-02,  1.4708e-01,  1.4555e-01,\n",
            "        -7.6356e-02, -1.1798e-01, -6.8436e-02, -1.4852e-01,  6.9748e-02,\n",
            "         1.7908e-01,  2.2027e-02, -6.7122e-03,  1.4381e-01,  1.4768e-02,\n",
            "         7.4228e-02,  9.1576e-02, -5.0980e-02,  4.8474e-02,  1.0151e-01,\n",
            "        -4.5996e-02, -1.4907e-01,  2.4645e-01, -1.1924e-01,  1.6786e-01,\n",
            "         1.8216e-01,  8.5208e-02,  8.0472e-02, -1.5424e-01,  8.1994e-02,\n",
            "         1.1618e-01,  5.7088e-02, -2.1864e-01, -4.9010e-02, -2.3876e-02,\n",
            "        -1.0694e-01, -1.2192e-01, -4.4551e-03,  3.4834e-02, -8.9230e-02,\n",
            "        -7.5313e-02, -5.1431e-02,  1.5149e-01,  5.8526e-02,  9.8156e-02,\n",
            "        -9.7105e-02, -5.8802e-02, -1.4918e-01,  6.5215e-02, -1.8360e-03,\n",
            "         1.1453e-01, -1.1283e-01,  4.0887e-02,  8.1122e-02, -5.6727e-02,\n",
            "        -1.4118e-02, -1.0808e-01,  1.7106e-01,  4.9626e-02, -1.5355e-03,\n",
            "        -8.2864e-03, -5.6729e-02, -1.0446e-01, -4.1445e-02, -4.0053e-02,\n",
            "         1.4747e-01,  8.5412e-02, -7.3055e-02,  1.2083e-01,  1.4570e-01,\n",
            "         2.2836e-02, -2.6668e-02, -1.7888e-01,  5.6933e-02,  2.4022e-01,\n",
            "        -1.0186e-01, -1.9470e-02,  4.6686e-02, -2.6427e-02,  4.4006e-02,\n",
            "        -7.6057e-02, -2.2970e-02, -4.2101e-02,  1.4392e-01,  1.9567e-01,\n",
            "         3.9883e-02, -6.1621e-02,  6.3166e-02,  8.8150e-02,  2.8684e-02,\n",
            "        -2.2381e-02, -4.8474e-02,  1.0759e-01, -6.0641e-02, -1.1333e-02,\n",
            "        -6.7506e-02,  6.3947e-02, -3.0790e-02,  8.5970e-02,  5.6495e-04,\n",
            "        -1.0443e-01, -3.3453e-03,  5.0482e-02,  9.9996e-02,  2.1924e-02,\n",
            "        -2.3806e-02, -8.5347e-02,  1.8461e-01,  2.5132e-01,  9.3074e-03,\n",
            "        -5.9489e-03, -7.4376e-02,  2.4062e-01, -1.9151e-01, -8.8412e-02,\n",
            "        -3.0076e-02,  1.0223e-01, -9.7259e-02, -7.8604e-02,  8.9790e-02,\n",
            "         1.7023e-01,  3.4346e-02, -1.8615e-01, -1.1572e-01,  5.3740e-02,\n",
            "        -2.7845e-02, -3.9332e-04,  1.4473e-01, -1.9975e-01, -2.7854e-02,\n",
            "        -1.7088e-01, -5.2817e-02, -8.8876e-02, -7.6400e-02, -1.1815e-03,\n",
            "        -1.1624e-01,  1.7853e-04, -7.1541e-02, -1.1272e-01,  2.3165e-02,\n",
            "        -2.4635e-01,  1.4007e-01, -1.9912e-01,  1.8767e-03,  2.9502e-02,\n",
            "        -8.2256e-02,  2.0154e-01, -1.5255e-02, -1.0092e-01,  2.0998e-01,\n",
            "        -8.4555e-03, -1.8804e-02, -1.8192e-01,  5.0750e-02,  8.0411e-04,\n",
            "         2.8559e-02,  3.5745e-02, -3.8656e-02, -1.3862e-01,  2.5281e-02,\n",
            "        -3.8741e-02, -2.1807e-02, -6.2179e-02, -1.0322e-02, -8.4237e-03,\n",
            "        -1.7700e-01, -4.8896e-04, -8.5108e-02, -7.8329e-02, -5.6370e-02,\n",
            "        -2.7666e-02,  8.0702e-02, -6.7246e-02, -1.1318e-01,  2.0682e-01,\n",
            "         2.2862e-02, -9.1160e-02,  1.4442e-01,  1.3897e-02, -6.5989e-02,\n",
            "         5.9997e-02, -1.2904e-01, -3.2373e-02,  7.3903e-02, -7.0979e-02,\n",
            "         6.5872e-02,  1.9544e-01, -2.2625e-01,  3.4376e-02,  5.5713e-02,\n",
            "         2.5990e-01, -1.3391e-02, -2.7929e-01,  1.2608e-01,  1.2624e-01,\n",
            "        -1.5179e-01, -4.2668e-02, -4.2330e-02,  9.7372e-02, -9.1761e-02,\n",
            "         3.3388e-02,  1.1265e-01,  1.2504e-01, -8.2842e-02, -1.8991e-03,\n",
            "        -8.5983e-02, -1.1525e-01, -4.9854e-03,  1.7742e-01, -4.4870e-02,\n",
            "        -3.1884e-02, -2.1566e-01, -2.0671e-02,  6.5619e-02, -2.1527e-02,\n",
            "        -1.4687e-01, -1.9536e-01,  1.2476e-01, -3.8109e-02,  1.9415e-02,\n",
            "        -5.4711e-02, -1.2965e-02,  1.0556e-01,  1.8525e-01,  4.9762e-02,\n",
            "        -1.1389e-01, -9.7584e-02, -5.5143e-02,  8.5269e-02, -6.5015e-03,\n",
            "         1.5214e-01,  5.4997e-02, -1.2799e-01,  2.0707e-01, -1.2470e-01,\n",
            "        -5.9045e-02, -1.5397e-01, -2.0213e-03,  8.0341e-02, -2.3379e-01,\n",
            "        -1.3491e-01,  1.3384e-01,  1.3879e-02,  6.3540e-03,  1.3197e-01,\n",
            "        -7.8278e-02, -3.5108e-02,  1.0526e-02, -9.8268e-02, -4.5878e-02,\n",
            "         3.9374e-02, -1.6751e-02, -1.2688e-01,  1.0284e-01,  3.7151e-02,\n",
            "         7.7278e-02,  4.1174e-02,  6.3969e-03, -2.1001e-01,  1.5255e-01,\n",
            "         8.1912e-02, -2.2405e-01, -1.1129e-01, -2.6656e-01,  1.5458e-01,\n",
            "        -1.1068e-01,  2.9796e-02, -1.1597e-03,  1.0896e-01, -4.4657e-02,\n",
            "         1.2860e-01,  9.6089e-02, -1.5375e-01,  2.6346e-03,  1.3673e-01,\n",
            "         9.3475e-02, -1.6537e-01, -1.5489e-01,  1.1409e-01,  1.3875e-02,\n",
            "         1.4026e-01,  3.0207e-02, -4.2037e-02,  4.7132e-02, -7.0935e-03,\n",
            "         1.0424e-01, -2.2236e-01,  6.7144e-02,  4.7069e-02, -9.7564e-02,\n",
            "         6.5416e-03, -9.0673e-02,  6.2849e-02, -2.8064e-02,  1.1262e-01,\n",
            "        -5.8557e-02, -1.6729e-01,  1.1651e-02,  1.1089e-01, -8.2330e-02,\n",
            "         6.2233e-02,  9.5419e-02, -1.8926e-02, -2.2538e-01, -9.2714e-02,\n",
            "         1.3026e-01, -1.5024e-01,  9.1258e-02, -3.2634e-03,  7.7241e-03,\n",
            "        -6.1266e-04, -1.1789e-01,  2.7634e-02,  8.6118e-02, -4.2776e-02,\n",
            "        -1.5394e-01,  9.4931e-03, -3.8786e-02,  2.7560e-02,  5.8960e-02,\n",
            "         2.8014e-01,  9.2307e-03,  1.6722e-01,  9.8658e-02,  5.9392e-02,\n",
            "        -1.7874e-01, -2.4083e-02, -2.2822e-01, -8.0073e-02, -1.7852e-01,\n",
            "         4.3531e-02,  2.0651e-01,  1.8420e-01, -1.4412e-01, -4.3704e-02,\n",
            "         3.4742e-02,  7.6245e-02, -9.8178e-02,  9.5277e-02,  1.1052e-01,\n",
            "         1.9907e-01,  1.0532e-01, -8.9643e-02])\n",
            "5. Token: '</s>' → tensor([ 2.6797e-02,  4.7723e-02, -3.9051e-02, -9.2507e-02,  1.3535e-01,\n",
            "        -5.2497e-02, -1.7274e-02,  7.0830e-02,  1.4262e-02,  7.0442e-02,\n",
            "        -5.5257e-02,  1.7104e-02,  2.9592e-02, -4.8385e-02, -1.0964e-01,\n",
            "        -6.9463e-03, -3.7840e-02, -6.1971e-02, -3.4402e-02,  8.0619e-02,\n",
            "         4.8083e-02,  9.3204e-02, -9.3535e-02, -1.6312e-01, -7.0953e-02,\n",
            "        -6.3823e-02, -1.0249e-01, -1.2871e-01, -7.2178e-02, -1.0181e-01,\n",
            "        -1.1572e-01, -4.4551e-02, -1.0548e-01, -1.7286e-02,  9.7103e-03,\n",
            "         1.0834e-01, -4.2335e-02, -3.3018e-02, -4.1012e-02, -1.0411e-01,\n",
            "         6.3686e-02, -4.3389e-02, -5.7748e-03, -5.4696e-03,  1.6833e-01,\n",
            "        -4.9215e-02,  6.2964e-02, -5.9667e-02, -1.5269e-01,  6.1092e-02,\n",
            "         1.5029e-01, -3.5830e-02, -1.1648e-01,  4.1359e-02,  1.8278e-02,\n",
            "        -1.4780e-02, -3.1131e-02, -1.0945e-01, -1.4486e-02,  1.6478e-01,\n",
            "        -9.8716e-02,  5.0035e-02,  8.8849e-02, -3.8897e-02, -6.4561e-02,\n",
            "        -8.3300e-02, -4.0619e-02, -7.1902e-02, -5.8435e-02,  1.4412e-02,\n",
            "         1.5547e-01,  1.3527e-01,  5.8213e-02,  3.2647e-02,  1.4568e-01,\n",
            "        -4.2722e-02, -2.0842e-02, -6.4192e-02, -3.3670e-02,  3.6488e-02,\n",
            "         2.1557e-02, -5.3946e-02, -6.5447e-02, -1.0594e-01, -5.6533e-02,\n",
            "        -4.8481e-02, -1.3106e-01, -1.6384e-02,  2.9698e-02, -1.4180e-01,\n",
            "         8.3459e-02, -8.8555e-04,  8.2487e-02,  5.4784e-05,  4.7157e-03,\n",
            "        -3.0340e-02, -1.2592e-01,  1.8537e-01,  4.2592e-02,  1.4966e-02,\n",
            "         1.4532e-01, -4.2521e-02, -8.5302e-02,  1.5532e-05,  2.7168e-02,\n",
            "        -5.1198e-02,  3.1611e-02, -1.0108e-01,  1.2298e-02, -1.6386e-02,\n",
            "        -1.2483e-03,  3.1722e-02,  9.0643e-03,  5.2488e-02,  5.0148e-02,\n",
            "         4.1742e-02, -2.2243e-02,  4.2242e-02,  1.4194e-02, -4.8088e-02,\n",
            "        -7.4396e-03, -3.8369e-02, -7.3812e-02,  8.0411e-02,  1.9146e-01,\n",
            "        -2.4317e-02, -1.8959e-03, -5.3002e-02, -1.9229e-01, -1.8480e-02,\n",
            "        -4.8198e-02,  2.2659e-02,  1.2028e-01, -9.3161e-02,  1.8495e-02,\n",
            "        -4.1681e-02, -4.5904e-02,  1.9886e-02,  1.0128e-01, -5.2316e-03,\n",
            "        -4.0350e-02, -6.2822e-02, -1.1552e-01,  1.2791e-01, -5.1324e-02,\n",
            "         4.9843e-02,  3.6065e-02,  2.3351e-02, -4.7827e-02,  5.0747e-02,\n",
            "         1.2875e-03, -2.3145e-02, -6.4446e-02, -1.6703e-01,  2.0275e-01,\n",
            "        -5.1204e-02, -5.9667e-02,  1.1520e-01,  1.1386e-01, -1.1073e-01,\n",
            "        -5.2010e-02,  2.1987e-02, -1.0746e-01,  2.8474e-02, -1.0986e-01,\n",
            "         4.4115e-01,  1.6941e-01,  3.3470e-02,  1.5775e-01, -6.9441e-02,\n",
            "        -1.2986e-01, -1.9600e-01,  6.7167e-02, -4.6733e-02, -1.2646e-01,\n",
            "         6.1668e-02,  1.6944e-01, -1.9460e-01, -3.4909e-02, -3.3357e-02,\n",
            "         1.3538e-01,  6.0313e-02,  9.7367e-02, -9.1100e-02, -6.8251e-02,\n",
            "        -3.1669e-02, -1.5416e-02,  1.2404e-01, -2.0575e-01,  4.9366e-02,\n",
            "         8.6202e-02,  4.1249e-02,  1.0175e-01,  1.0808e-01, -7.7711e-02,\n",
            "         1.8301e-03, -2.2081e-02, -1.2402e-01, -3.9913e-02,  8.2821e-02,\n",
            "        -7.7463e-02,  5.4035e-02, -7.3634e-02, -4.1522e-02,  1.2340e-01,\n",
            "        -1.8683e-02,  6.3222e-02,  1.1575e-01, -4.4639e-02,  2.9045e-02,\n",
            "        -1.4237e-02, -3.6098e-02,  3.1524e-02, -4.3892e-02, -5.6232e-02,\n",
            "         3.0914e-02, -3.1807e-02, -4.2561e-02, -4.3309e-02,  9.7923e-02,\n",
            "         6.3659e-03, -8.8256e-02,  3.5502e-02,  4.9449e-02,  2.5224e-02,\n",
            "        -2.4783e-02, -3.4639e-02,  1.0646e-01,  1.3435e-02, -1.0048e-01,\n",
            "         8.4899e-02,  1.4899e-01, -1.1744e-01,  1.9668e-01, -6.4526e-02,\n",
            "         3.3532e-02, -8.7082e-02, -1.9017e-02,  8.0855e-02,  5.1333e-02,\n",
            "         1.3255e-02, -3.9844e-02,  3.8094e-02,  3.2757e-02, -7.2702e-02,\n",
            "        -9.2160e-02,  2.8364e-02,  2.3241e-02, -2.1749e-02,  4.0573e-02,\n",
            "        -6.4679e-02, -7.5236e-02,  7.7556e-04, -1.7320e-02, -1.0461e-01,\n",
            "         4.9934e-02, -3.8369e-02, -3.0958e-02,  1.4105e-02, -1.3873e-01,\n",
            "         2.6113e-02, -1.2124e-01, -6.6009e-02, -1.1099e-02, -4.0796e-02,\n",
            "         8.3567e-02, -1.3364e-01, -2.8701e-02,  1.0962e-01, -1.6185e-01,\n",
            "        -1.2520e-01,  4.9420e-02, -1.8029e-02, -1.3625e-01, -8.4009e-02,\n",
            "         4.8592e-02, -1.9402e-02, -9.0149e-03,  1.1381e-02, -1.0516e-01,\n",
            "         3.3524e-02,  8.4099e-02,  1.3886e-01,  1.0873e-01,  8.4054e-02,\n",
            "        -5.6471e-02,  4.7722e-02, -3.0416e-02,  1.8463e-01,  8.4007e-02,\n",
            "         1.1905e-01,  1.3962e-02, -9.0380e-03, -1.4119e-01,  1.0834e-01,\n",
            "         1.1148e-01,  3.1636e-02, -6.3730e-02,  2.8982e-02, -7.9295e-02,\n",
            "        -2.7565e-02,  2.5702e-04, -8.6434e-02, -3.9765e-02,  7.1801e-02,\n",
            "         6.5788e-02,  4.4007e-02, -1.8946e-02, -1.0839e-01, -1.3553e-02,\n",
            "        -2.6076e-02, -7.5607e-03, -2.1956e-02, -4.3027e-02,  9.6698e-02,\n",
            "        -1.7844e-01,  1.6016e-01, -2.4264e-02, -1.1620e-01, -2.2921e-03,\n",
            "         2.8382e-03,  1.2777e-01, -7.9222e-02, -4.4576e-02,  5.0844e-02,\n",
            "        -3.1747e-03, -1.1721e-01, -2.5408e-02, -5.9995e-03,  5.4181e-02,\n",
            "         9.3749e-02,  4.1553e-02,  1.0744e-01,  5.3701e-02, -5.0426e-02,\n",
            "        -8.4777e-02, -3.4497e-02,  7.1298e-02, -8.2508e-02, -9.0110e-02,\n",
            "         4.5260e-02,  4.6383e-02, -1.5444e-01,  2.2550e-01, -1.5785e-01,\n",
            "         6.8543e-02, -6.3923e-01, -4.2362e-02,  1.2417e-01, -5.7707e-02,\n",
            "        -4.5089e-02,  1.3711e-02, -8.1885e-02, -5.1940e-02,  2.5397e-02,\n",
            "        -5.0426e-02,  4.1296e-02, -1.8833e-01,  9.6137e-02, -1.8812e-02,\n",
            "        -7.9657e-02,  4.0907e-02,  8.6874e-03, -3.8821e-02,  2.0162e-01,\n",
            "        -9.0393e-02, -1.1267e-01,  1.7248e-02,  3.2794e-02,  9.0660e-04,\n",
            "         4.4110e-02, -1.6579e-01, -1.0493e-01,  8.2334e-02, -1.0458e-01,\n",
            "         7.8410e-02, -1.9021e-02,  3.6308e-02,  6.6778e-02, -1.6100e-01,\n",
            "        -3.1673e-02, -1.5045e-01, -1.0128e-01,  5.1386e-02, -2.5867e-02,\n",
            "         1.2246e-01, -1.6873e-01, -8.8632e-02, -6.1250e-02, -2.7625e-05,\n",
            "         5.6747e-02, -1.3357e-02,  4.2164e-01, -4.8020e-02, -2.3776e-02,\n",
            "        -2.5744e-03,  4.9604e-02, -5.8781e-02,  1.3672e-01, -1.2841e-02,\n",
            "        -5.9900e-02,  7.0663e-02, -9.0054e-02, -2.9165e-03, -1.2354e-02,\n",
            "         1.2163e-01, -3.5573e-02, -3.0545e-02,  1.5245e-01, -4.8429e-01,\n",
            "         1.5666e-02,  1.1370e-01, -5.1165e-02, -1.7369e-01,  6.5288e-04,\n",
            "        -4.8127e-02, -1.1868e-01, -6.1805e-02,  8.9795e-03,  2.1081e-02,\n",
            "         8.8492e-02,  7.5589e-02, -3.9523e-03, -7.1488e-02,  3.4870e-02,\n",
            "        -3.0382e-02, -1.6605e-01,  3.4707e-02,  1.9267e-01,  2.2464e-01,\n",
            "        -1.0722e-01, -6.9390e-02,  1.0513e-01, -1.4454e-01,  6.2208e-03,\n",
            "         8.4220e-02,  4.8730e-02, -1.6909e-02,  6.7954e-02, -3.9279e-02,\n",
            "         3.7322e-02,  8.2352e-02, -1.1835e-02,  6.7257e-02, -1.2044e-02,\n",
            "        -1.3482e-01, -1.1328e-01,  9.8762e-02, -1.2352e-01,  5.9046e-02,\n",
            "         8.0784e-02,  5.9437e-02,  3.9975e-02, -1.9711e-01,  1.0388e-01,\n",
            "         1.3454e-01,  2.1247e-02, -1.1720e-01,  7.7138e-02,  9.2450e-02,\n",
            "        -1.2943e-02, -2.5949e-02, -3.0103e-02, -6.8372e-02,  5.2465e-02,\n",
            "         1.0849e-03, -5.0156e-02,  2.1549e-01, -9.9940e-03,  1.7578e-01,\n",
            "        -3.1979e-02,  8.2981e-03, -1.2516e-01,  8.0132e-02, -8.5748e-03,\n",
            "         5.9816e-02, -7.5101e-02, -1.0066e-01,  4.8227e-03, -2.3583e-02,\n",
            "         1.0039e-01, -2.1931e-02, -5.1288e-02,  5.3838e-02, -3.3773e-02,\n",
            "        -2.8195e-02, -5.6500e-02, -5.1387e-02,  6.5282e-03,  1.0317e-02,\n",
            "         1.3581e-01,  3.9050e-02, -8.7593e-02,  6.1538e-02, -6.6564e-02,\n",
            "         6.7551e-02,  3.7893e-02, -1.2981e-01,  4.4378e-02,  2.7617e-01,\n",
            "        -5.3462e-02, -5.6443e-03,  3.5197e-03, -1.1795e-01,  8.1216e-02,\n",
            "        -6.1097e-02,  3.2746e-02, -6.2001e-02,  1.3410e-01, -2.1524e-02,\n",
            "         5.9807e-02, -1.0672e-03,  4.2723e-02, -4.9041e-02,  5.8005e-02,\n",
            "         2.9330e-02,  5.0411e-02,  3.0554e-02,  5.8134e-02, -4.2086e-02,\n",
            "        -4.1259e-02,  3.7707e-02,  5.9535e-03,  2.7183e-02,  1.4845e-03,\n",
            "        -4.5626e-02,  1.2353e-01, -1.6107e-02,  7.8391e-02, -6.8927e-03,\n",
            "        -4.6883e-02, -7.8347e-02,  1.5229e-02,  3.2315e-01, -2.6619e-02,\n",
            "         5.1125e-02, -5.1757e-03,  3.4239e-01, -9.1758e-02, -1.3000e-01,\n",
            "         6.3041e-02,  7.6027e-02, -8.0109e-02, -7.9570e-02,  3.7832e-02,\n",
            "         1.1995e-01, -6.4045e-02, -1.2508e-01, -1.0865e-01,  2.7220e-02,\n",
            "         2.2399e-02,  1.6802e-02,  1.3874e-01, -1.2199e-01,  2.6003e-03,\n",
            "        -4.5447e-02, -9.2256e-02, -4.9671e-02, -1.6076e-02,  2.5823e-02,\n",
            "        -2.3225e-02,  7.4643e-05, -4.3699e-02, -1.7206e-01, -3.8551e-02,\n",
            "        -2.0400e-01,  1.5178e-01, -1.5440e-01, -1.0254e-01, -1.3233e-01,\n",
            "        -8.5989e-03,  7.6808e-02,  3.8144e-02,  9.5195e-03,  1.3878e-01,\n",
            "         1.8037e-02, -1.0384e-01, -8.9901e-02, -4.5947e-02,  1.2501e-03,\n",
            "         3.4456e-02,  2.0387e-02, -6.6717e-03, -8.2880e-02,  9.7414e-03,\n",
            "        -7.7048e-03, -3.6101e-02, -3.3796e-02, -8.5938e-02, -1.2940e-02,\n",
            "        -1.3671e-01, -2.6529e-01, -5.3387e-02, -2.5309e-02, -1.3653e-02,\n",
            "        -3.6314e-02,  3.7515e-02, -1.4486e-01, -5.3510e-02,  1.2401e-01,\n",
            "         4.2258e-02, -1.6186e-01,  6.9101e-02,  9.5314e-02,  3.4508e-02,\n",
            "        -5.9079e-03, -9.9227e-02, -7.1473e-02, -8.3061e-03,  1.0624e-01,\n",
            "        -3.3900e-02,  5.6498e-02, -8.3353e-02,  1.1883e-02, -5.5764e-02,\n",
            "         1.6952e-01, -7.0470e-03, -2.0640e-01,  2.0263e-02,  8.5096e-02,\n",
            "        -1.0075e-01, -3.9609e-02,  6.2262e-02, -8.8654e-02,  3.1038e-02,\n",
            "         9.7496e-02,  4.1432e-02,  3.0820e-02,  2.6473e-02,  8.4093e-03,\n",
            "         1.8741e-02, -1.5722e-01,  4.8409e-02,  3.0349e-01,  2.0661e-02,\n",
            "        -4.7533e-02, -1.6252e-01, -2.7753e-02,  3.8808e-02, -1.3732e-02,\n",
            "        -1.2124e-01, -1.6992e-01,  8.8673e-02, -4.0568e-02, -9.6859e-02,\n",
            "        -7.9066e-02, -4.8016e-02,  9.5782e-02, -4.3389e-02,  8.2244e-02,\n",
            "        -2.1304e-02, -1.0105e-01, -4.7799e-02,  7.7471e-02, -8.1942e-02,\n",
            "         1.3974e-01,  4.1050e-02, -9.0169e-02,  1.6339e-01, -4.7369e-02,\n",
            "        -1.4434e-01, -8.9414e-03, -3.2821e-02,  9.5710e-02, -1.5353e-01,\n",
            "        -1.0274e-01, -6.3836e-02,  2.8665e-02,  7.4419e-02,  1.4287e-01,\n",
            "        -4.3761e-02, -1.9138e-03,  6.5912e-02, -1.6250e-01,  3.6672e-03,\n",
            "         1.2235e-01,  2.9389e-02, -3.6914e-02,  1.6963e-01,  1.0278e-01,\n",
            "         6.5078e-02,  6.8355e-02, -5.1752e-02, -1.2289e-01, -1.5892e-03,\n",
            "         7.2101e-02, -6.3389e-02,  3.4138e-02, -3.2423e-01,  3.0135e-02,\n",
            "        -1.1015e-01,  1.1486e-01, -2.5823e-02,  1.6299e-01,  8.8061e-02,\n",
            "         1.0669e-01,  8.6624e-02, -1.7512e-01,  2.9871e-03,  1.3699e-01,\n",
            "         1.3716e-01, -7.9938e-02, -1.7418e-01,  7.7882e-02, -3.2768e-02,\n",
            "         2.5770e-01,  8.1478e-02, -3.4199e-02, -9.1479e-02, -1.2982e-02,\n",
            "         8.8468e-02, -9.4663e-02, -2.7655e-02,  2.5182e-02, -4.4402e-02,\n",
            "        -4.6447e-02, -3.6015e-02,  5.8211e-02, -1.4424e-01,  2.4982e-01,\n",
            "        -1.0816e-01, -1.0297e-01, -1.3124e-02,  1.1628e-01, -6.7576e-02,\n",
            "        -7.4340e-02,  1.9791e-02, -1.5178e-02, -2.0339e-01, -1.4099e-01,\n",
            "         1.6308e-01, -1.3645e-01, -5.3939e-02, -1.3933e-02,  2.5944e-02,\n",
            "        -1.1974e-01, -7.3136e-02,  4.7315e-02, -1.4572e-01, -1.8762e-02,\n",
            "        -5.3828e-02,  8.7873e-02,  5.7812e-03,  6.0498e-02,  1.0117e-03,\n",
            "         2.0174e-01,  5.8848e-02,  1.6804e-01,  1.5183e-01,  1.9094e-02,\n",
            "        -4.1420e-02, -9.8699e-02, -1.7772e-01, -9.0827e-02, -1.6343e-01,\n",
            "         2.8654e-02,  6.1742e-02, -1.9104e-02, -8.2688e-02, -4.1678e-02,\n",
            "        -4.7843e-03,  8.9849e-02, -4.2425e-02,  5.0067e-02,  4.2179e-02,\n",
            "        -9.7638e-02,  9.9212e-03, -6.1739e-02])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_embedding_dimensions():\n",
        "    \"\"\"Understand the trade-off between embedding size and model capacity\"\"\"\n",
        "\n",
        "    vocab_size = 32100\n",
        "    embedding_dims = [128, 256, 512, 1024]\n",
        "\n",
        "    print(\"Embedding Dimension Analysis:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for dim in embedding_dims:\n",
        "        params = vocab_size * dim\n",
        "        print(f\"Embedding dim: {dim}\")\n",
        "        print(f\"  Parameters: {params:,} ({params/1e6:.2f}M)\")\n",
        "        print(f\"  Memory (float32): {params*4/1024/1024:.2f} MB\")\n",
        "        print(f\"  Expressiveness: {'Low' if dim < 256 else 'Medium' if dim < 512 else 'High'}\")\n",
        "        print()\n",
        "\n",
        "analyze_embedding_dimensions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KBUot6WS2A8",
        "outputId": "3051940e-3493-4f54-e4b7-3fda6521f6a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding Dimension Analysis:\n",
            "--------------------------------------------------\n",
            "Embedding dim: 128\n",
            "  Parameters: 4,108,800 (4.11M)\n",
            "  Memory (float32): 15.67 MB\n",
            "  Expressiveness: Low\n",
            "\n",
            "Embedding dim: 256\n",
            "  Parameters: 8,217,600 (8.22M)\n",
            "  Memory (float32): 31.35 MB\n",
            "  Expressiveness: Medium\n",
            "\n",
            "Embedding dim: 512\n",
            "  Parameters: 16,435,200 (16.44M)\n",
            "  Memory (float32): 62.70 MB\n",
            "  Expressiveness: High\n",
            "\n",
            "Embedding dim: 1024\n",
            "  Parameters: 32,870,400 (32.87M)\n",
            "  Memory (float32): 125.39 MB\n",
            "  Expressiveness: High\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Dummy GPT-like model class with only what's needed\n",
        "class TinyGPTEmbedDemo(nn.Module):\n",
        "    def __init__(self, vocab_size=500, block_size=8, n_embd=16):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, n_embd)\n",
        "        self.pos_emb = nn.Embedding(block_size, n_embd)\n",
        "\n",
        "    def forward_embedding_demo(self, idx, tokenizer=None):\n",
        "        B, T = idx.size()\n",
        "        pos = torch.arange(0, T, device=idx.device)\n",
        "\n",
        "        token_emb = self.token_emb(idx)\n",
        "        pos_emb = self.pos_emb(pos)\n",
        "\n",
        "        x = token_emb + pos_emb\n",
        "\n",
        "        print(f\"Token embeddings shape     : {token_emb.shape}\")\n",
        "        print(f\"Positional embeddings shape: {pos_emb.shape}\")\n",
        "        print(f\"Combined embeddings shape  : {x.shape}\")\n",
        "\n",
        "        print(\"\\nToken-wise Embedding Breakdown for first sentence:\")\n",
        "        for t in range(T):\n",
        "            token_id = idx[0, t].item()\n",
        "            token_text = tokenizer.decode([token_id]) if tokenizer else f\"id {token_id}\"\n",
        "            token_vec = token_emb[0, t][:5].detach().cpu().numpy()\n",
        "            pos_vec = pos_emb[t][:5].detach().cpu().numpy()\n",
        "            comb_vec = x[0, t][:5].detach().cpu().numpy()\n",
        "            print(f\"Token: {token_text:10s} | Pos {t} | TokEmb[:5]: {token_vec.round(3)} | PosEmb[:5]: {pos_vec.round(3)} | Sum[:5]: {comb_vec.round(3)}\")\n",
        "\n",
        "        return x\n",
        "\n",
        "# Instantiate dummy model\n",
        "demo_model = TinyGPTEmbedDemo()\n",
        "\n",
        "# Create dummy token index input (batch_size=1, sequence_length=5)\n",
        "# This simulates tokenized: [\"রহিম\", \"সারাকে\", \"ভালোবাসে\", \"।\", \"<eos>\"] <eos> is - end of sentence token\n",
        "# with made-up token IDs: [101, 202, 303, 0, 1]\n",
        "idx = torch.tensor([[101, 202, 303, 0, 1]])\n",
        "\n",
        "# Call the demo function (no tokenizer needed)\n",
        "demo_model.forward_embedding_demo(idx)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yxt7X4SIJlww",
        "outputId": "b3c00c66-da58-4537-9a1d-2f3bd2086b25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token embeddings shape     : torch.Size([1, 5, 16])\n",
            "Positional embeddings shape: torch.Size([5, 16])\n",
            "Combined embeddings shape  : torch.Size([1, 5, 16])\n",
            "\n",
            "Token-wise Embedding Breakdown for first sentence:\n",
            "Token: id 101     | Pos 0 | TokEmb[:5]: [ 0.261 -0.947 -0.061  0.229 -0.232] | PosEmb[:5]: [-1.238 -1.337  0.152 -0.278  0.225] | Sum[:5]: [-0.977 -2.284  0.091 -0.049 -0.007]\n",
            "Token: id 202     | Pos 1 | TokEmb[:5]: [ 1.446  0.477 -0.523  0.158  0.084] | PosEmb[:5]: [-1.307 -0.742  0.306  1.892  1.573] | Sum[:5]: [ 0.139 -0.265 -0.216  2.05   1.657]\n",
            "Token: id 303     | Pos 2 | TokEmb[:5]: [ 0.493 -0.367 -0.407  0.773 -1.619] | PosEmb[:5]: [-0.502 -0.891 -0.399  0.387  0.464] | Sum[:5]: [-0.009 -1.258 -0.806  1.16  -1.155]\n",
            "Token: id 0       | Pos 3 | TokEmb[:5]: [-0.126 -2.268  1.583  0.491  1.04 ] | PosEmb[:5]: [ 0.204 -1.783  0.837 -0.895 -1.391] | Sum[:5]: [ 0.077 -4.051  2.419 -0.404 -0.351]\n",
            "Token: id 1       | Pos 4 | TokEmb[:5]: [ 1.267 -0.234  0.301 -2.464 -0.069] | PosEmb[:5]: [-0.874 -0.968  0.382 -0.28  -2.293] | Sum[:5]: [ 0.393 -1.203  0.683 -2.744 -2.362]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.9767, -2.2841,  0.0915, -0.0494, -0.0068,  0.7625,  2.4597,\n",
              "          -0.6108,  1.4289, -2.2406, -2.2759,  0.4544, -1.9511, -0.0480,\n",
              "          -0.3135, -3.0045],\n",
              "         [ 0.1387, -0.2648, -0.2164,  2.0504,  1.6567, -0.8299,  1.1838,\n",
              "          -0.0691, -1.2788, -2.2135, -0.4820, -0.3062,  1.5035, -1.5401,\n",
              "          -1.0214, -1.3381],\n",
              "         [-0.0090, -1.2578, -0.8055,  1.1597, -1.1545, -0.7994, -0.6574,\n",
              "          -1.2865,  0.7352, -0.7159,  1.7051,  1.0261, -1.2717, -1.4344,\n",
              "          -3.3872, -0.5389],\n",
              "         [ 0.0770, -4.0510,  2.4193, -0.4042, -0.3512, -0.9160,  1.4652,\n",
              "           0.0859, -1.1043,  1.8672, -0.2632, -2.1267,  0.7742,  1.5271,\n",
              "          -0.8423,  1.8176],\n",
              "         [ 0.3930, -1.2027,  0.6832, -2.7440, -2.3618, -2.2166,  0.2539,\n",
              "           3.3841,  1.1149,  0.1560,  2.1672, -0.3326, -1.9245, -0.0571,\n",
              "           1.7174, -2.2777]]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def visualize_dropout():\n",
        "    \"\"\"Show how dropout randomly zeros out neurons\"\"\"\n",
        "\n",
        "    # Simulate a layer with 20 neurons\n",
        "    neurons = np.ones(20)\n",
        "    dropout_rate = 0.1\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "    # Before dropout\n",
        "    ax1.bar(range(20), neurons, color='lightblue')\n",
        "    ax1.set_title('Before Dropout (Training)')\n",
        "    ax1.set_ylabel('Neuron Activation')\n",
        "    ax1.set_xlabel('Neuron Index')\n",
        "\n",
        "    # After dropout\n",
        "    mask = np.random.random(20) > dropout_rate\n",
        "    neurons_dropped = neurons * mask\n",
        "    colors = ['red' if not m else 'lightblue' for m in mask]\n",
        "\n",
        "    ax2.bar(range(20), neurons_dropped, color=colors)\n",
        "    ax2.set_title(f'After Dropout (rate={dropout_rate})')\n",
        "    ax2.set_ylabel('Neuron Activation')\n",
        "    ax2.set_xlabel('Neuron Index')\n",
        "\n",
        "    dropped_count = sum(~mask)\n",
        "    ax2.text(15, 0.8, f'{dropped_count} neurons\\ndropped out',\n",
        "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\"))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Dropped {dropped_count}/{len(neurons)} neurons ({100*dropped_count/len(neurons):.1f}%)\")\n",
        "\n",
        "visualize_dropout()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "pGz6j1UFQMWi",
        "outputId": "48143bc2-4716-4809-f9be-c881dbc5055f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropped 1/20 neurons (5.0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_dropout_rates():\n",
        "    \"\"\"Understand different dropout rates and their effects\"\"\"\n",
        "\n",
        "    dropout_rates = [0.0, 0.1, 0.2, 0.5]\n",
        "    effects = [\n",
        "        \"No regularization - High overfitting risk\",\n",
        "        \"Light regularization - Good for most tasks\",\n",
        "        \"Medium regularization - For complex models\",\n",
        "        \"Heavy regularization - May underfit\"\n",
        "    ]\n",
        "\n",
        "    print(\"Dropout Rate Analysis:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    for rate, effect in zip(dropout_rates, effects):\n",
        "        print(f\"Rate {rate}: {effect}\")\n",
        "        if rate == 0.2:\n",
        "            print(\"  → Our choice for the Bangla GPT model ✓\")\n",
        "        print()\n",
        "\n",
        "analyze_dropout_rates()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sa_WdumTQV_8",
        "outputId": "c0c52a8b-0e9c-4957-e896-5d843a71a416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropout Rate Analysis:\n",
            "----------------------------------------\n",
            "Rate 0.0: No regularization - High overfitting risk\n",
            "\n",
            "Rate 0.1: Light regularization - Good for most tasks\n",
            "\n",
            "Rate 0.2: Medium regularization - For complex models\n",
            "  → Our choice for the Bangla GPT model ✓\n",
            "\n",
            "Rate 0.5: Heavy regularization - May underfit\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "d_model = 512\n",
        "d_k = 64\n",
        "\n",
        "# These are layers, so they are learnable\n",
        "W_Q = nn.Linear(d_model, d_k)\n",
        "W_K = nn.Linear(d_model, d_k)\n",
        "W_V = nn.Linear(d_model, d_k)\n",
        "\n",
        "# Input X ∈ [batch, seq_len, d_model]\n",
        "X = torch.rand(32, 20, d_model)  # e.g., batch of 32, 20 tokens\n",
        "\n",
        "Q = W_Q(X)  # Output shape: [32, 20, 64]\n",
        "K = W_K(X)\n",
        "V = W_V(X)\n",
        "\n",
        "print(\"Q =\" ,Q)\n",
        "print(\"K =\", K)\n",
        "print(\"V =\", V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRq_KnUnER64",
        "outputId": "d7e5a656-ab70-453f-c67c-42853dae792e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q = tensor([[[-0.1806,  0.3008, -0.2343,  ...,  0.1113,  0.0210,  0.5369],\n",
            "         [-0.1182,  0.1680, -0.2614,  ..., -0.1736,  0.2154,  0.5788],\n",
            "         [-0.2108,  0.1857,  0.0861,  ..., -0.1360, -0.0550,  0.9361],\n",
            "         ...,\n",
            "         [-0.1334,  0.2544, -0.0686,  ...,  0.0380, -0.1911,  0.7548],\n",
            "         [-0.0717, -0.0115, -0.4418,  ...,  0.0424, -0.0298,  0.6161],\n",
            "         [-0.1430,  0.6127, -0.3463,  ..., -0.2843, -0.0684,  0.6312]],\n",
            "\n",
            "        [[-0.0050,  0.4406, -0.5498,  ..., -0.0569,  0.1731,  0.5754],\n",
            "         [ 0.1778,  0.7387,  0.0163,  ..., -0.0256, -0.1470,  0.4508],\n",
            "         [-0.1669,  0.4388, -0.4627,  ..., -0.2139,  0.1164,  0.7095],\n",
            "         ...,\n",
            "         [-0.2152,  0.5129, -0.3884,  ...,  0.0276,  0.0677,  0.4153],\n",
            "         [-0.1196,  0.7459, -0.1744,  ..., -0.0205, -0.1331,  0.5624],\n",
            "         [-0.0992,  0.5029, -0.3945,  ..., -0.0775,  0.0134,  0.7324]],\n",
            "\n",
            "        [[-0.1110,  0.1212, -0.3109,  ...,  0.2346, -0.0935,  0.6420],\n",
            "         [-0.0619,  0.4796, -0.2846,  ...,  0.4099,  0.4188,  0.4301],\n",
            "         [-0.0867,  0.6596, -0.1809,  ..., -0.0607,  0.0263,  0.5442],\n",
            "         ...,\n",
            "         [ 0.2367,  0.6488, -0.3497,  ...,  0.1115, -0.1538,  0.6139],\n",
            "         [-0.3625,  0.2420, -0.4164,  ..., -0.0014, -0.0274,  0.7695],\n",
            "         [-0.1487,  0.3545, -0.3809,  ..., -0.0962,  0.0808,  0.8285]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0983,  0.3795, -0.1924,  ...,  0.2088, -0.0745,  0.6936],\n",
            "         [-0.2167,  0.6093, -0.4633,  ...,  0.2806,  0.0824,  0.6423],\n",
            "         [-0.0524,  0.2479, -0.4594,  ...,  0.2801,  0.0633,  0.4635],\n",
            "         ...,\n",
            "         [ 0.0448,  0.3766, -0.2200,  ...,  0.1196, -0.1168,  0.3436],\n",
            "         [-0.1725,  0.3248, -0.2155,  ...,  0.0911,  0.1055,  0.6946],\n",
            "         [-0.4663,  0.4006, -0.3696,  ...,  0.1387,  0.1619,  0.7904]],\n",
            "\n",
            "        [[-0.2603,  0.3558, -0.2975,  ...,  0.4204, -0.0430,  0.5626],\n",
            "         [ 0.2927,  0.5994, -0.2640,  ..., -0.0825, -0.0453,  0.4547],\n",
            "         [-0.0820,  0.3626, -0.2684,  ..., -0.1756, -0.0078,  0.2805],\n",
            "         ...,\n",
            "         [-0.2616,  0.4490, -0.4740,  ..., -0.1894,  0.0106,  0.3545],\n",
            "         [-0.1187,  0.3110, -0.2957,  ...,  0.0980, -0.1037,  0.5770],\n",
            "         [-0.2912,  0.4901, -0.4656,  ...,  0.1850, -0.0304,  0.8371]],\n",
            "\n",
            "        [[-0.2716,  0.2787, -0.4287,  ...,  0.0658,  0.2917,  0.2479],\n",
            "         [-0.1116,  0.4584, -0.4156,  ...,  0.1092, -0.0112,  0.4768],\n",
            "         [-0.0984,  0.3201, -0.2616,  ..., -0.1454,  0.1232,  0.4702],\n",
            "         ...,\n",
            "         [-0.1618,  0.6629, -0.1971,  ..., -0.0277,  0.2690,  0.5945],\n",
            "         [-0.2292,  0.4777, -0.1007,  ...,  0.1837,  0.0322,  0.8307],\n",
            "         [-0.0851,  0.6613,  0.0079,  ..., -0.0436, -0.2251,  0.6178]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "K = tensor([[[-2.8613e-02,  8.9686e-01, -4.3758e-02,  ...,  3.8006e-01,\n",
            "          -9.1929e-02, -1.1883e-01],\n",
            "         [-1.6029e-01,  7.1012e-01, -1.8970e-01,  ...,  1.1134e-01,\n",
            "           3.7164e-01,  1.2896e-02],\n",
            "         [ 1.3968e-01,  5.8263e-01,  8.0370e-02,  ..., -5.7512e-02,\n",
            "           2.6256e-02,  2.1924e-01],\n",
            "         ...,\n",
            "         [ 2.0893e-01,  7.5512e-01,  3.2662e-01,  ..., -1.5620e-01,\n",
            "           2.4265e-01, -7.4601e-03],\n",
            "         [-4.3725e-03,  6.8083e-01,  1.3569e-01,  ..., -4.8552e-02,\n",
            "           1.3910e-01,  1.2339e-01],\n",
            "         [-7.7731e-02,  9.0385e-01,  8.5383e-02,  ...,  9.2686e-04,\n",
            "           5.4847e-02,  2.7853e-01]],\n",
            "\n",
            "        [[ 8.6351e-02,  8.9484e-01,  2.7959e-01,  ..., -1.7945e-01,\n",
            "          -2.9215e-02,  1.0634e-01],\n",
            "         [-2.2097e-01,  7.5320e-01, -1.1013e-01,  ..., -1.8791e-01,\n",
            "          -2.2274e-01, -9.3306e-02],\n",
            "         [ 2.2943e-01,  6.9631e-01, -6.1486e-02,  ..., -2.7615e-01,\n",
            "           1.9504e-01,  2.1662e-01],\n",
            "         ...,\n",
            "         [ 6.9494e-02,  5.4988e-01, -3.7750e-02,  ..., -7.3118e-02,\n",
            "          -3.6516e-02,  2.4164e-01],\n",
            "         [ 2.0246e-02,  8.5011e-01, -1.9404e-03,  ..., -2.2136e-01,\n",
            "          -8.0008e-02,  1.5998e-01],\n",
            "         [-2.6270e-02,  7.6917e-01,  1.0897e-01,  ..., -8.1426e-02,\n",
            "          -4.1186e-02, -7.3510e-03]],\n",
            "\n",
            "        [[-1.2806e-01,  8.3922e-01,  1.5790e-01,  ..., -3.0728e-01,\n",
            "           1.3515e-01, -1.4152e-01],\n",
            "         [-5.0501e-02,  8.1955e-01,  2.9968e-01,  ..., -8.9245e-03,\n",
            "          -8.8761e-02,  3.6949e-01],\n",
            "         [-2.2078e-01,  9.6313e-01,  1.9549e-01,  ..., -6.8514e-02,\n",
            "          -5.9801e-02, -2.8412e-01],\n",
            "         ...,\n",
            "         [ 2.4747e-01,  7.3013e-01,  2.0636e-01,  ..., -1.5129e-01,\n",
            "           2.5652e-01,  3.5402e-01],\n",
            "         [ 3.5017e-01,  6.0069e-01,  1.9620e-01,  ...,  4.3839e-02,\n",
            "           2.8891e-01,  2.6587e-01],\n",
            "         [-1.9339e-01,  8.4934e-01,  2.1620e-02,  ..., -3.6492e-02,\n",
            "           2.8574e-01,  2.9852e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.8429e-01,  6.0074e-01, -3.6460e-02,  ..., -1.2634e-01,\n",
            "           2.1280e-01,  1.0778e-01],\n",
            "         [-9.4810e-02,  5.3254e-01,  1.3957e-01,  ...,  2.3705e-01,\n",
            "          -1.3819e-01,  2.9297e-01],\n",
            "         [ 1.9277e-01,  4.5498e-01,  1.1179e-01,  ...,  2.3142e-01,\n",
            "          -1.9288e-01, -2.2762e-01],\n",
            "         ...,\n",
            "         [ 1.0768e-01,  8.3339e-01,  2.0388e-01,  ..., -6.3483e-02,\n",
            "           5.2918e-02, -1.4169e-01],\n",
            "         [ 9.7071e-02,  7.0075e-01, -1.3035e-01,  ...,  9.3835e-02,\n",
            "           1.6092e-01,  2.8651e-01],\n",
            "         [ 4.9447e-02,  6.8686e-01,  1.8536e-01,  ...,  1.0563e-01,\n",
            "           1.3202e-01,  3.4553e-01]],\n",
            "\n",
            "        [[ 1.2466e-01,  6.1895e-01,  2.7813e-01,  ..., -2.1813e-01,\n",
            "           2.6089e-02,  3.1712e-02],\n",
            "         [-8.6900e-02,  8.8986e-01, -1.6178e-01,  ..., -2.2033e-01,\n",
            "           4.4977e-01,  8.6412e-02],\n",
            "         [ 1.1883e-02,  7.2158e-01, -2.4539e-02,  ...,  2.0758e-01,\n",
            "           5.0428e-02,  4.2079e-02],\n",
            "         ...,\n",
            "         [-8.6321e-02,  8.4044e-01,  3.8586e-01,  ..., -4.7620e-02,\n",
            "           1.7947e-02, -2.3917e-02],\n",
            "         [ 3.4405e-01,  6.1016e-01,  1.3318e-01,  ..., -1.0760e-01,\n",
            "          -8.4348e-02,  9.6108e-02],\n",
            "         [ 5.6458e-03,  5.2574e-01,  2.7537e-01,  ...,  1.1392e-02,\n",
            "           1.0149e-01, -6.4103e-02]],\n",
            "\n",
            "        [[ 6.9854e-02,  6.1378e-01,  2.0298e-01,  ..., -1.5616e-01,\n",
            "           1.1534e-01, -2.8031e-01],\n",
            "         [ 1.2768e-01,  5.4203e-01, -5.0907e-02,  ...,  6.4667e-02,\n",
            "           2.2690e-01, -2.3412e-01],\n",
            "         [-1.2139e-01,  3.9813e-01,  2.1981e-01,  ...,  4.5936e-03,\n",
            "           1.6724e-01,  1.5250e-01],\n",
            "         ...,\n",
            "         [-4.4811e-02,  7.6950e-01,  3.2898e-01,  ...,  2.4413e-01,\n",
            "          -2.4998e-01,  7.2974e-02],\n",
            "         [-4.3994e-02,  7.4493e-01, -9.2865e-02,  ...,  3.6276e-02,\n",
            "          -1.3193e-01, -2.3134e-01],\n",
            "         [-6.6953e-03,  7.3482e-01, -2.7882e-04,  ..., -5.7881e-02,\n",
            "          -2.2624e-01, -1.0511e-01]]], grad_fn=<ViewBackward0>)\n",
            "V = tensor([[[ 4.1840e-01,  3.2254e-01, -4.3572e-01,  ..., -3.8245e-01,\n",
            "          -7.2629e-01,  9.2694e-02],\n",
            "         [ 1.0767e-01, -9.2447e-03, -1.3546e-01,  ..., -7.7868e-02,\n",
            "          -5.1771e-01, -1.5803e-01],\n",
            "         [ 1.9586e-01,  1.9649e-01, -5.3770e-01,  ..., -3.4320e-01,\n",
            "          -4.6622e-01, -3.3356e-02],\n",
            "         ...,\n",
            "         [ 2.0060e-01,  4.3719e-01, -1.3100e-01,  ..., -3.0166e-01,\n",
            "          -3.9216e-01, -1.8768e-01],\n",
            "         [-1.2058e-01, -4.9435e-02, -3.6002e-01,  ..., -2.3013e-01,\n",
            "          -3.8251e-01,  1.5708e-01],\n",
            "         [ 5.3003e-02,  4.7329e-01, -4.0308e-01,  ..., -1.0019e-01,\n",
            "          -7.6620e-01, -9.7212e-02]],\n",
            "\n",
            "        [[-2.3348e-01, -1.1056e-01, -3.2531e-01,  ..., -4.2005e-01,\n",
            "          -8.1777e-01, -3.0315e-01],\n",
            "         [-1.6808e-01,  1.9938e-01, -2.4412e-01,  ..., -7.0916e-02,\n",
            "          -4.7773e-01, -7.5336e-02],\n",
            "         [-1.7385e-02,  1.2455e-01, -1.9586e-01,  ...,  8.8874e-02,\n",
            "          -4.8129e-01,  1.7279e-01],\n",
            "         ...,\n",
            "         [ 6.0886e-03,  5.7940e-01, -3.8840e-01,  ..., -1.8118e-01,\n",
            "          -8.5317e-01,  2.5518e-01],\n",
            "         [ 1.0652e-02,  5.7188e-02, -5.2265e-01,  ..., -5.3830e-01,\n",
            "          -7.6697e-01, -1.9162e-01],\n",
            "         [-1.4988e-01, -6.1936e-02, -4.9518e-01,  ..., -5.0628e-02,\n",
            "          -9.7827e-01,  1.8508e-01]],\n",
            "\n",
            "        [[ 1.3543e-01,  4.0050e-01, -8.3319e-02,  ..., -3.4440e-01,\n",
            "          -5.9428e-01,  1.3837e-01],\n",
            "         [-2.1928e-01,  4.5505e-01, -4.0699e-01,  ..., -8.4872e-02,\n",
            "          -7.4479e-01,  1.6636e-02],\n",
            "         [ 2.9011e-01, -3.4494e-03, -3.9958e-01,  ..., -3.3303e-01,\n",
            "          -7.7636e-01,  8.6849e-02],\n",
            "         ...,\n",
            "         [ 1.0497e-02,  4.6596e-01, -1.6450e-01,  ..., -1.9490e-01,\n",
            "          -5.0110e-01,  4.9339e-03],\n",
            "         [ 3.5422e-02,  3.4559e-01, -4.6105e-01,  ..., -3.4843e-01,\n",
            "          -6.1493e-01,  1.5961e-01],\n",
            "         [ 1.0154e-01,  2.3163e-01, -4.2429e-01,  ..., -3.0637e-01,\n",
            "          -8.4014e-01,  1.5811e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.3833e-02,  9.2688e-02, -3.0807e-01,  ..., -1.2282e-02,\n",
            "          -6.3705e-01,  3.1001e-01],\n",
            "         [ 1.6067e-01,  1.0374e-01, -4.5810e-01,  ..., -5.5237e-01,\n",
            "          -5.7706e-01,  1.7864e-01],\n",
            "         [-6.7700e-02,  2.6395e-01, -4.3696e-01,  ..., -4.8328e-01,\n",
            "          -3.8047e-01,  1.5946e-01],\n",
            "         ...,\n",
            "         [ 1.3833e-01,  2.9257e-01, -1.5989e-01,  ..., -4.2764e-01,\n",
            "          -5.4878e-01,  2.3538e-02],\n",
            "         [ 2.4453e-01,  5.0253e-01, -5.4847e-01,  ..., -2.2041e-01,\n",
            "          -2.6754e-01,  3.5677e-01],\n",
            "         [-2.5324e-01,  2.5423e-01, -2.7750e-01,  ..., -4.4751e-01,\n",
            "          -6.6289e-01,  1.0537e-01]],\n",
            "\n",
            "        [[-1.1165e-01,  5.5633e-01, -2.8383e-01,  ..., -4.5542e-01,\n",
            "          -5.4581e-01,  6.4137e-02],\n",
            "         [-4.1631e-02,  4.3861e-01, -1.5860e-01,  ..., -2.7380e-01,\n",
            "          -4.0378e-01, -6.5240e-02],\n",
            "         [-1.2450e-02,  1.5640e-01, -4.2623e-01,  ..., -1.2824e-01,\n",
            "          -4.7608e-01,  1.7491e-02],\n",
            "         ...,\n",
            "         [ 4.4389e-02,  2.2971e-01, -1.1742e-01,  ..., -4.3712e-01,\n",
            "          -4.5102e-01,  4.4450e-02],\n",
            "         [ 1.6807e-02,  5.4481e-01, -6.2528e-01,  ..., -1.7603e-01,\n",
            "          -7.1403e-01, -8.9921e-04],\n",
            "         [-8.8537e-03,  3.8564e-01, -2.9576e-01,  ..., -1.6149e-01,\n",
            "          -4.6790e-01, -1.6681e-01]],\n",
            "\n",
            "        [[ 2.2420e-01,  2.4602e-01, -5.8869e-01,  ..., -2.8477e-01,\n",
            "          -6.2545e-01,  2.8428e-01],\n",
            "         [ 4.4901e-01,  2.9228e-02, -3.2739e-01,  ..., -2.8713e-01,\n",
            "          -6.9512e-01,  3.3644e-01],\n",
            "         [ 9.1999e-02, -1.0257e-01, -4.4517e-01,  ..., -6.4187e-01,\n",
            "          -5.3966e-01,  3.1499e-01],\n",
            "         ...,\n",
            "         [-1.1346e-01,  1.7809e-01, -5.5132e-01,  ..., -3.1989e-01,\n",
            "          -4.4237e-01,  2.7730e-01],\n",
            "         [ 2.9431e-01,  3.0853e-02, -3.3513e-01,  ..., -2.9224e-01,\n",
            "          -5.9155e-01,  1.8087e-02],\n",
            "         [-3.1231e-01, -2.8548e-02, -3.6014e-01,  ..., -2.9008e-01,\n",
            "          -4.4173e-01,  2.5404e-01]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_head_dimensions():\n",
        "    \"\"\"Understand how embedding dimension is split across heads\"\"\"\n",
        "\n",
        "    n_embd = 512    # Total embedding dimension\n",
        "    n_head = 8      # Number of attention heads\n",
        "\n",
        "    head_dim = n_embd // n_head\n",
        "\n",
        "    print(\"Multi-Head Dimension Calculation\")\n",
        "    print(\"=\" * 40)\n",
        "    print(f\"Total embedding dimension (n_embd): {n_embd}\")\n",
        "    print(f\"Number of heads (n_head): {n_head}\")\n",
        "    print(f\"Dimension per head: {n_embd} ÷ {n_head} = {head_dim}\")\n",
        "    print()\n",
        "\n",
        "    print(\"Why this matters:\")\n",
        "    print(f\"• Each head processes {head_dim}D representations\")\n",
        "    print(f\"• All heads combined = {n_head} × {head_dim} = {n_embd}D\")\n",
        "    print(\"• Different heads can specialize in different patterns\")\n",
        "    print(\"• Parallel computation across heads\")\n",
        "    print()\n",
        "\n",
        "    # Verify our model configuration\n",
        "    assert n_embd % n_head == 0, \"n_embd must be divisible by n_head!\"\n",
        "    print(\"Our configuration is valid!\")\n",
        "\n",
        "calculate_head_dimensions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1htdcrfOjpa6",
        "outputId": "529b8404-a955-428f-aa15-1d9b0fe72673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multi-Head Dimension Calculation\n",
            "========================================\n",
            "Total embedding dimension (n_embd): 512\n",
            "Number of heads (n_head): 8\n",
            "Dimension per head: 512 ÷ 8 = 64\n",
            "\n",
            "Why this matters:\n",
            "• Each head processes 64D representations\n",
            "• All heads combined = 8 × 64 = 512D\n",
            "• Different heads can specialize in different patterns\n",
            "• Parallel computation across heads\n",
            "\n",
            "Our configuration is valid!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "uJAjH8na-vda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class CausalSelfAttentionDetailed(nn.Module):\n",
        "    \"\"\"Detailed implementation with step-by-step explanations\"\"\"\n",
        "\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "\n",
        "        # Store configuration\n",
        "        self.n_head = cfg.n_head\n",
        "        self.n_embd = cfg.n_embd\n",
        "        self.head_dim = cfg.n_embd // cfg.n_head\n",
        "\n",
        "        # Linear transformations for Q, K, V\n",
        "        self.query = nn.Linear(cfg.n_embd, cfg.n_embd, bias=False)\n",
        "        self.key = nn.Linear(cfg.n_embd, cfg.n_embd, bias=False)\n",
        "        self.value = nn.Linear(cfg.n_embd, cfg.n_embd, bias=False)\n",
        "\n",
        "        # Output projection\n",
        "        self.proj = nn.Linear(cfg.n_embd, cfg.n_embd, bias=False)\n",
        "\n",
        "        # Dropout layers\n",
        "        self.attn_drop = nn.Dropout(cfg.dropout)\n",
        "        self.resid_drop = nn.Dropout(cfg.dropout)\n",
        "\n",
        "        # Causal mask\n",
        "        mask = torch.tril(torch.ones(cfg.block_size, cfg.block_size))\n",
        "        mask = mask.view(1, 1, cfg.block_size, cfg.block_size)\n",
        "        self.register_buffer(\"mask\", mask)\n",
        "\n",
        "        print(\" CausalSelfAttention initialized!\")\n",
        "        print(f\"   Heads: {self.n_head}\")\n",
        "        print(f\"   Embedding dim: {self.n_embd}\")\n",
        "        print(f\"   Head dim: {self.head_dim}\")\n",
        "\n",
        "    def forward(self, x, verbose=False):\n",
        "        B, T, C = x.shape  # Batch, Time, Channels\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\n Forward pass:\")\n",
        "            print(f\"   Input shape: {x.shape}\")\n",
        "\n",
        "        # Step 1: Generate Q, K, V\n",
        "        q = self.query(x)  # (B, T, C)\n",
        "        k = self.key(x)    # (B, T, C)\n",
        "        v = self.value(x)  # (B, T, C)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"   Q, K, V shapes: {q.shape}\")\n",
        "\n",
        "        # Step 2: Reshape for multi-head attention\n",
        "        # (B, T, C) → (B, T, n_head, head_dim) → (B, n_head, T, head_dim)\n",
        "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"   Reshaped Q, K, V: {q.shape}\")\n",
        "\n",
        "        # Step 3: Scaled dot-product attention\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"   Attention scores: {att.shape}\")\n",
        "\n",
        "        # Step 4: Apply causal mask\n",
        "        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "\n",
        "        if verbose:\n",
        "            print(\"   Applied causal mask\")\n",
        "\n",
        "        # Step 5: Softmax and dropout\n",
        "        att = torch.softmax(att, dim=-1)\n",
        "        att = self.attn_drop(att)\n",
        "\n",
        "        # Step 6: Apply attention to values\n",
        "        y = att @ v  # (B, n_head, T, head_dim)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"   Attention output: {y.shape}\")\n",
        "\n",
        "        # Step 7: Concatenate heads\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"   Concatenated heads: {y.shape}\")\n",
        "\n",
        "        # Step 8: Final projection and dropout\n",
        "        y = self.resid_drop(self.proj(y))\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"   Final output: {y.shape}\")\n",
        "\n",
        "        return y\n",
        "\n",
        "# Test the implementation\n",
        "def test_causal_attention():\n",
        "    \"\"\"Test our attention implementation with sample data\"\"\"\n",
        "\n",
        "    from dataclasses import dataclass\n",
        "\n",
        "    @dataclass\n",
        "    class TestConfig:\n",
        "        n_embd: int = 512\n",
        "        n_head: int = 8\n",
        "        block_size: int = 128\n",
        "        dropout: float = 0.1\n",
        "\n",
        "    cfg = TestConfig()\n",
        "    attention = CausalSelfAttentionDetailed(cfg)\n",
        "\n",
        "    # Sample input: batch_size=2, seq_len=10, embed_dim=512\n",
        "    x = torch.randn(2, 10, 512)\n",
        "\n",
        "    print(\"\\n Testing Causal Self-Attention:\")\n",
        "    output = attention(x, verbose=True)\n",
        "\n",
        "    print(f\"\\n Test passed! Output shape matches input: {output.shape}\")\n",
        "\n",
        "test_causal_attention()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tnXUC-qlHfc",
        "outputId": "e1635e8a-260c-4222-96d3-e5e606db6fc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " CausalSelfAttention initialized!\n",
            "   Heads: 8\n",
            "   Embedding dim: 512\n",
            "   Head dim: 64\n",
            "\n",
            " Testing Causal Self-Attention:\n",
            "\n",
            " Forward pass:\n",
            "   Input shape: torch.Size([2, 10, 512])\n",
            "   Q, K, V shapes: torch.Size([2, 10, 512])\n",
            "   Reshaped Q, K, V: torch.Size([2, 8, 10, 64])\n",
            "   Attention scores: torch.Size([2, 8, 10, 10])\n",
            "   Applied causal mask\n",
            "   Attention output: torch.Size([2, 8, 10, 64])\n",
            "   Concatenated heads: torch.Size([2, 10, 512])\n",
            "   Final output: torch.Size([2, 10, 512])\n",
            "\n",
            " Test passed! Output shape matches input: torch.Size([2, 10, 512])\n"
          ]
        }
      ]
    }
  ]
}